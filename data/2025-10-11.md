<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 86]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Reasoning by Exploration: A Unified Approach to Retrieval and Generation over Graphs](https://arxiv.org/abs/2510.07484)
*Haoyu Han,Kai Guo,Harry Shomer,Yu Wang,Yucheng Chu,Hang Li,Li Ma,Jiliang Tang*

Main category: cs.IR

TL;DR: 本文提出RoE方法，将图结构推理统一为探索过程，通过两阶段训练显著提升LLM在图推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的方法在图推理中存在检索质量限制生成效果的问题，且现有检索器对未见图泛化能力差，限制了实际应用。

Method: 提出Reasoning by Exploration (RoE)方法，将图推理统一为探索过程：LLM逐步选择节点和边进行探索，构建推理路径并生成答案。采用两阶段训练：监督微调+强化学习。

Result: 在基准数据集上的实验表明，RoE相比基线方法实现了显著的整体改进，并能有效泛化到未见过的图结构。

Conclusion: RoE通过统一检索和生成过程，解决了传统两阶段方法在图推理中的局限性，提供了更有效的图结构推理解决方案。

Abstract: Reasoning over structured graphs remains a fundamental challenge for Large
Language Models (LLMs), particularly when scaling to large graphs. Existing
approaches typically follow the retrieval-augmented generation (RAG) paradigm:
first retrieving subgraphs relevant to the query and then generating answers
conditioned on the retrieved subgraphs. However, such two-phase pipelines often
struggle to faithfully incorporate graph structure, since the generation
process is ultimately constrained by the quality and completeness of the
retrieved subgraph. Although many advanced retrievers have been proposed
recently to mitigate this issue, they are usually tailored to the training
graphs and generalize poorly to unseen graphs, which limits their practical
applicability. In this work, we propose Reasoning by Exploration (RoE), a novel
approach that unifies retrieval and generation by framing reasoning over graphs
as a process of graph exploration. At each step, the LLM selects candidate
nodes and edges to explore, gradually constructing reasoning paths and
generating answers along the way. To enable effective exploration, RoE is
trained in two stages: supervised fine-tuning (SFT) on gold reasoning paths,
followed by reinforcement learning (RL) to enhance exploration effectiveness
and generalization. Experiments on benchmark datasets demonstrate that RoE
achieves substantial overall improvements over baselines, while also
generalizing effectively to unseen graphs.

</details>


### [2] [Queries Are Not Alone: Clustering Text Embeddings for Video Search](https://arxiv.org/abs/2510.07720)
*Peyang Liu,Xi Wang,Ziqiang Cui,Wei Ye*

Main category: cs.IR

TL;DR: 本文提出了一种新颖的视频文本聚类（VTC）框架，通过聚类文本查询来增强视频检索性能，引入Sweeper模块消除聚类噪声，并开发VTC-Att机制动态调整注意力，在五个公共数据集上超越了现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 传统视频检索方法主要依赖文本查询与视频元数据的直接匹配，难以弥合文本描述与视频多面性内容之间的语义鸿沟，因此需要更先进的系统来提升检索效果。

Method: 提出视频文本聚类（VTC）框架，采用独特的聚类机制对相关查询进行分组，引入Sweeper模块识别和减轻聚类中的噪声，并开发视频文本聚类注意力（VTC-Att）机制动态调整聚类内的注意力焦点。

Result: 在五个公共数据集上的实验表明，所提出的模型超越了现有的最先进模型，证明了该框架在视频检索任务中的有效性。

Conclusion: VTC框架通过聚类文本查询并动态调整注意力，成功提升了视频检索的语义理解能力，为视频内容检索提供了更有效的解决方案。

Abstract: The rapid proliferation of video content across various platforms has
highlighted the urgent need for advanced video retrieval systems. Traditional
methods, which primarily depend on directly matching textual queries with video
metadata, often fail to bridge the semantic gap between text descriptions and
the multifaceted nature of video content. This paper introduces a novel
framework, the Video-Text Cluster (VTC), which enhances video retrieval by
clustering text queries to capture a broader semantic scope. We propose a
unique clustering mechanism that groups related queries, enabling our system to
consider multiple interpretations and nuances of each query. This clustering is
further refined by our innovative Sweeper module, which identifies and
mitigates noise within these clusters. Additionally, we introduce the
Video-Text Cluster-Attention (VTC-Att) mechanism, which dynamically adjusts
focus within the clusters based on the video content, ensuring that the
retrieval process emphasizes the most relevant textual features. Further
experiments have demonstrated that our proposed model surpasses existing
state-of-the-art models on five public datasets.

</details>


### [3] [Who Stole Your Data? A Method for Detecting Unauthorized RAG Theft](https://arxiv.org/abs/2510.07728)
*Peiyang Liu,Ziqiang Cui,Di Liang,Wei Ye*

Main category: cs.IR

TL;DR: 本文提出了一种针对检索增强生成（RAG）系统的双重水印保护框架，包括专门的数据集RPD和语义/词汇层面的水印技术，以解决RAG系统中的数据盗用问题。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）虽然能减少大语言模型的幻觉和过时信息问题，但也为大规模未经授权的数据盗用提供了便利，需要保护知识产权。

Method: 开发了RPD数据集用于RAG抄袭检测，并设计了双重水印系统（语义层和词汇层），结合询问者-侦探框架使用统计假设检验。

Result: 实验表明该方法在不同查询量、防御提示和检索参数下都有效，且能抵抗对抗性规避技术。

Conclusion: 这项工作为检索增强AI系统中的知识产权保护建立了基础框架。

Abstract: Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by
mitigating hallucinations and outdated information issues, yet simultaneously
facilitates unauthorized data appropriation at scale. This paper addresses this
challenge through two key contributions. First, we introduce RPD, a novel
dataset specifically designed for RAG plagiarism detection that encompasses
diverse professional domains and writing styles, overcoming limitations in
existing resources. Second, we develop a dual-layered watermarking system that
embeds protection at both semantic and lexical levels, complemented by an
interrogator-detective framework that employs statistical hypothesis testing on
accumulated evidence. Extensive experimentation demonstrates our approach's
effectiveness across varying query volumes, defense prompts, and retrieval
parameters, while maintaining resilience against adversarial evasion
techniques. This work establishes a foundational framework for intellectual
property protection in retrieval-augmented AI systems.

</details>


### [4] [PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations](https://arxiv.org/abs/2510.07784)
*Ruining He,Lukasz Heldt,Lichan Hong,Raghunandan Keshavan,Shifan Mao,Nikhil Mehta,Zhengyang Su,Alicia Tsai,Yueqi Wang,Shao-Chuan Wang,Xinyang Yi,Lexi Baugher,Baykal Cakici,Ed Chi,Cristos Goodrow,Ningren Han,He Ma,Romer Rosales,Abby Van Soest,Devansh Tandon,Su-Lin Wu,Weilong Yang,Yilin Zheng*

Main category: cs.IR

TL;DR: PLUM是一个将预训练大语言模型适配到工业级推荐任务的框架，通过项目语义ID、领域继续预训练和任务特定微调，在YouTube视频推荐中相比传统嵌入表方法取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的序列建模能力和世界知识为推荐系统提供了新的建模范式，PLUM旨在利用这些能力改进工业级推荐任务。

Method: PLUM框架包含三个核心组件：使用语义ID进行项目标记化、在领域特定数据上继续预训练、以及针对推荐目标进行任务特定微调，特别关注生成式检索方法。

Result: 在大规模内部视频推荐数据集上的实验表明，PLUM相比基于大型嵌入表的生产模型在检索性能上取得显著提升，并成功部署到YouTube的数十亿用户。

Conclusion: PLUM框架成功展示了将预训练LLMs适配到工业级推荐系统的可行性，通过语义ID和生成式检索等方法实现了性能突破，为推荐系统领域提供了新的技术路径。

Abstract: Large Language Models (LLMs) pose a new paradigm of modeling and computation
for information tasks. Recommendation systems are a critical application domain
poised to benefit significantly from the sequence modeling capabilities and
world knowledge inherent in these large models. In this paper, we introduce
PLUM, a framework designed to adapt pre-trained LLMs for industry-scale
recommendation tasks. PLUM consists of item tokenization using Semantic IDs,
continued pre-training (CPT) on domain-specific data, and task-specific
fine-tuning for recommendation objectives. For fine-tuning, we focus
particularly on generative retrieval, where the model is directly trained to
generate Semantic IDs of recommended items based on user context. We conduct
comprehensive experiments on large-scale internal video recommendation
datasets. Our results demonstrate that PLUM achieves substantial improvements
for retrieval compared to a heavily-optimized production model built with large
embedding tables. We also present a scaling study for the model's retrieval
performance, our learnings about CPT, a few enhancements to Semantic IDs, along
with an overview of the training and inference methods that enable launching
this framework to billions of users in YouTube.

</details>


### [5] [Generation and annotation of item usage scenarios in e-commerce using large language models](https://arxiv.org/abs/2510.07885)
*Madoka Hagiri,Kazushi Okamoto,Koki Karube,Kei Harada,Atsushi Shibata*

Main category: cs.IR

TL;DR: 该论文探索使用大语言模型生成物品使用场景来构建互补推荐系统，通过人工标注验证了约85%的生成场景是合理的。


<details>
  <summary>Details</summary>
Motivation: 传统基于历史数据的互补推荐方法依赖统计共现，但互补关系具有主观性和个体差异性，难以从历史数据中准确推断。作者关注驱动物品组合的底层使用情境。

Method: 基于"人们通过想象具体使用场景来选择互补物品"的假设，探索使用大语言模型生成物品使用场景作为构建互补推荐系统的起点，并通过人工标注评估生成场景的合理性。

Result: 人工标注结果显示，约85%的LLM生成场景被判定为合理的，表明大语言模型能够有效生成真实的物品使用场景。

Conclusion: 大语言模型能够生成合理的物品使用场景，为构建基于使用情境的互补推荐系统提供了可行起点。

Abstract: Complementary recommendations suggest combinations of useful items that play
important roles in e-commerce. However, complementary relationships are often
subjective and vary among individuals, making them difficult to infer from
historical data. Unlike conventional history-based methods that rely on
statistical co-occurrence, we focus on the underlying usage context that
motivates item combinations. We hypothesized that people select complementary
items by imagining specific usage scenarios and identifying the needs in such
situations. Based on this idea, we explored the use of large language models
(LLMs) to generate item usage scenarios as a starting point for constructing
complementary recommendation systems. First, we evaluated the plausibility of
LLM-generated scenarios through manual annotation. The results demonstrated
that approximately 85% of the generated scenarios were determined to be
plausible, suggesting that LLMs can effectively generate realistic item usage
scenarios.

</details>


### [6] [VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents](https://arxiv.org/abs/2510.08109)
*Daniel Huwiler,Kurt Stockinger,Jonathan Fürst*

Main category: cs.IR

TL;DR: VersionRAG是一个版本感知的检索增强生成框架，通过层次图结构建模文档演化，在版本敏感问题上达到90%准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在处理版本化技术文档时存在严重缺陷，只能达到58-64%的准确率，无法进行时间有效性检查。

Method: 采用层次图结构捕获版本序列、内容边界和文档状态变化，通过意图分类进行查询路由，实现精确的版本感知过滤和变更跟踪。

Result: 在VersionQA基准测试中达到90%准确率，比朴素RAG(58%)和GraphRAG(64%)显著提升；在隐式变更检测上达到60%准确率，而基线方法几乎完全失败(0-10%)；索引时比GraphRAG减少97%的token使用量。

Conclusion: 该研究确立了版本化文档问答作为一个独立任务，并为未来研究提供了解决方案和基准测试。

Abstract: Retrieval-Augmented Generation (RAG) systems fail when documents evolve
through versioning-a ubiquitous characteristic of technical documentation.
Existing approaches achieve only 58-64% accuracy on version-sensitive
questions, retrieving semantically similar content without temporal validity
checks. We present VersionRAG, a version-aware RAG framework that explicitly
models document evolution through a hierarchical graph structure capturing
version sequences, content boundaries, and changes between document states.
During retrieval, VersionRAG routes queries through specialized paths based on
intent classification, enabling precise version-aware filtering and change
tracking. On our VersionQA benchmark-100 manually curated questions across 34
versioned technical documents-VersionRAG achieves 90% accuracy, outperforming
naive RAG (58%) and GraphRAG (64%). VersionRAG reaches 60% accuracy on implicit
change detection where baselines fail (0-10%), demonstrating its ability to
track undocumented modifications. Additionally, VersionRAG requires 97% fewer
tokens during indexing than GraphRAG, making it practical for large-scale
deployment. Our work establishes versioned document QA as a distinct task and
provides both a solution and benchmark for future research.

</details>


### [7] [Mobile Gamer Lifetime Value Prediction via Objective Decomposition and Reconstruction](https://arxiv.org/abs/2510.08281)
*Tianwei Li,Yu Zhao,Yunze Li,Sheng Li*

Main category: cs.IR

TL;DR: 本文提出了一种新的用户生命周期价值预测方法，通过目标分解和重构框架解决LTV分布挑战，在TapTap RTB广告系统中进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 互联网平台的实时竞价广告服务需要准确预测用户生命周期价值，但现有方法对异常值敏感，预测效果有限。

Method: 基于移动游戏玩家的应用内购买特征，设计模型先预测特定价格点的交易次数，然后从这些中间预测计算总支付金额。

Result: 在真实工业数据集上进行实验评估，并与最先进的ZILN模型进行在线A/B测试。

Conclusion: 提出的目标分解和重构框架能够有效解决LTV分布复杂性带来的预测挑战。

Abstract: For Internet platforms operating real-time bidding (RTB) advertising service,
a comprehensive understanding of user lifetime value (LTV) plays a pivotal role
in optimizing advertisement allocation efficiency and maximizing the return on
investment (ROI) for advertisement sponsors, thereby facilitating growth of
commercialization revenue for the platform. However, the inherent complexity of
user LTV distributions induces significant challenges in accurate LTV
prediction. Existing state-of-the-art works, which primarily focus on directly
learning the LTV distributions through well-designed loss functions, achieve
limited success due to their vulnerability to outliers. In this paper, we
proposed a novel LTV prediction method to address distribution challenges
through an objective decomposition and reconstruction framework. Briefly
speaking, based on the in-app purchase characteristics of mobile gamers, our
model was designed to first predict the number of transactions at specific
prices and then calculate the total payment amount from these intermediate
predictions. Our proposed model was evaluated through experiments on real-world
industrial dataset, and deployed on the TapTap RTB advertising system for
online A/B testing along with the state-of-the-art ZILN model.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children](https://arxiv.org/abs/2510.07320)
*Nelaka K. A. R,Peiris M. K. V,Liyanage R. P. B*

Main category: cs.LG

TL;DR: 本研究探讨了自闭症谱系障碍儿童在技能发展前的行为模式和情绪识别，通过纵向监测建立基线理解，并提出了针对信息技术领域应用的干预框架。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍严重影响个体的沟通能力、学习过程、行为和社交互动。虽然早期干预和定制教育策略对改善结果至关重要，但在理解和处理自闭症儿童技能发展前的细微行为模式和情绪识别方面存在关键差距。

Method: 采用纵向方法监测情绪和行为，通过详细分析随时间变化的行为趋势，建立对自闭症学生独特需求和挑战的基线理解。

Result: 提出了一个针对性的框架，用于开发满足已识别需求的应用和技术辅助工具，强调基于证据的干预方法。

Conclusion: 通过将重点转向早期行为模式识别，旨在培养更具包容性和支持性的学习环境，显著改善自闭症儿童的教育和发展轨迹。

Abstract: Autism Spectrum Disorder significantly influences the communication
abilities, learning processes, behavior, and social interactions of
individuals. Although early intervention and customized educational strategies
are critical to improving outcomes, there is a pivotal gap in understanding and
addressing nuanced behavioral patterns and emotional identification in autistic
children prior to skill development. This extended research delves into the
foundational step of recognizing and mapping these patterns as a prerequisite
to improving learning and soft skills. Using a longitudinal approach to monitor
emotions and behaviors, this study aims to establish a baseline understanding
of the unique needs and challenges faced by autistic students, particularly in
the Information Technology domain, where opportunities are markedly limited.
Through a detailed analysis of behavioral trends over time, we propose a
targeted framework for developing applications and technical aids designed to
meet these identified needs. Our research underscores the importance of a
sequential and evidence-based intervention approach that prioritizes a deep
understanding of each child's behavioral and emotional landscape as the basis
for effective skill development. By shifting the focus toward early
identification of behavioral patterns, we aim to foster a more inclusive and
supportive learning environment that can significantly improve the educational
and developmental trajectory of children with ASD.

</details>


### [9] [MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation](https://arxiv.org/abs/2510.07328)
*Md Zubair,Hao Zheng,Nussdorf Jonathan,Grayson W. Armstrong,Lucy Q. Shen,Gabriela Wilson,Yu Tian,Xingquan Zhu,Min Shi*

Main category: cs.LG

TL;DR: 本文提出MultiFair方法解决多模态医疗分类中的两个关键挑战：模态学习不平衡和群体性能不公平问题，通过双重梯度调制实现公平的多模态学习。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习模型存在两个关键问题：1）不同数据模态学习不均衡导致模型偏向某些模态；2）模型可能偏向某些人口群体导致性能不公平。这两个方面相互影响，造成不平衡且不公平的多模态学习。

Method: 提出MultiFair方法，采用双重梯度调制过程，在数据模态和群体两个层面上动态调节训练梯度的优化方向和幅度。

Result: 在两个多模态医疗数据集上的实验表明，MultiFair优于最先进的多模态学习和公平性学习方法。

Conclusion: MultiFair通过双重梯度调制有效解决了多模态医疗分类中的模态不平衡和群体不公平问题，实现了更公平可靠的多模态学习。

Abstract: Medical decision systems increasingly rely on data from multiple sources to
ensure reliable and unbiased diagnosis. However, existing multimodal learning
models fail to achieve this goal because they often ignore two critical
challenges. First, various data modalities may learn unevenly, thereby
converging to a model biased towards certain modalities. Second, the model may
emphasize learning on certain demographic groups causing unfair performances.
The two aspects can influence each other, as different data modalities may
favor respective groups during optimization, leading to both imbalanced and
unfair multimodal learning. This paper proposes a novel approach called
MultiFair for multimodal medical classification, which addresses these
challenges with a dual-level gradient modulation process. MultiFair dynamically
modulates training gradients regarding the optimization direction and magnitude
at both data modality and group levels. We conduct extensive experiments on two
multimodal medical datasets with different demographic groups. The results show
that MultiFair outperforms state-of-the-art multimodal learning and fairness
learning methods.

</details>


### [10] [Out-of-Distribution Generalization in Climate-Aware Yield Prediction with Earth Observation Data](https://arxiv.org/abs/2510.07350)
*Aditya Chakravarty*

Main category: cs.LG

TL;DR: 本文评估了两种深度学习模型（GNN-RNN和MMST-ViT）在作物产量预测中的跨区域泛化能力，发现GNN-RNN在跨区域转移中表现更优，而MMST-ViT在域外条件下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 气候变化正日益破坏农业系统，准确的作物产量预测对粮食安全至关重要。虽然深度学习模型在使用卫星和天气数据进行产量预测方面显示出潜力，但其跨地理区域和年份的泛化能力（对实际部署至关重要）尚未得到充分测试。

Method: 使用覆盖2017-2022年1200多个美国县的大规模CropNet数据集，在现实的域外条件下对GNN-RNN和MMST-ViT两种最先进模型进行基准测试。通过跨七个USDA农场资源区域的留一聚类交叉验证和年度预测场景，评估模型的跨区域可转移性。

Result: GNN-RNN在跨区域转移中表现出更好的泛化能力，而MMST-ViT在域内表现良好但在域外条件下性能急剧下降。Heartland和Northern Great Plains等区域显示出稳定的转移动态（大豆RMSE小于10 bu/acre），而Prairie Gateway在所有模型和作物中持续表现不佳（RMSE大于20 bu/acre）。GNN-RNN的训练速度比MMST-ViT快135倍（14分钟 vs. 31.5小时）。

Conclusion: 研究发现时空对齐（而不仅仅是模型复杂度或数据规模）是实现稳健泛化的关键，并强调需要透明的域外评估协议来确保公平可靠的气候感知农业预测。GNN-RNN由于其优越的泛化能力和训练效率，更适合可持续部署。

Abstract: Climate change is increasingly disrupting agricultural systems, making
accurate crop yield forecasting essential for food security. While deep
learning models have shown promise in yield prediction using satellite and
weather data, their ability to generalize across geographic regions and years -
critical for real-world deployment - remains largely untested. We benchmark two
state-of-the-art models, GNN-RNN and MMST-ViT, under realistic
out-of-distribution (OOD) conditions using the large-scale CropNet dataset
spanning 1,200+ U.S. counties from 2017-2022. Through leave-one-cluster-out
cross-validation across seven USDA Farm Resource Regions and year-ahead
prediction scenarios, we identify substantial variability in cross-region
transferability. GNN-RNN demonstrates superior generalization with positive
correlations under geographic shifts, while MMST-ViT performs well in-domain
but degrades sharply under OOD conditions. Regions like Heartland and Northern
Great Plains show stable transfer dynamics (RMSE less than 10 bu/acre for
soybean), whereas Prairie Gateway exhibits persistent underperformance (RMSE
greater than 20 bu/acre) across both models and crops, revealing structural
dissimilarities likely driven by semi-arid climate, irrigation patterns, and
incomplete spectral coverage. Beyond accuracy differences, GNN-RNN achieves
135x faster training than MMST-ViT (14 minutes vs. 31.5 hours), making it more
viable for sustainable deployment. Our findings underscore that
spatial-temporal alignment - not merely model complexity or data scale - is key
to robust generalization, and highlight the need for transparent OOD evaluation
protocols to ensure equitable and reliable climate-aware agricultural
forecasting.

</details>


### [11] [ConCuR: Conciseness Makes State-of-the-Art Kernel Generation](https://arxiv.org/abs/2510.07356)
*Lingcheng Kong,Jiateng Wei,Hanzhang Shen,Huan Wang*

Main category: cs.LG

TL;DR: 该论文提出了一个生成和筛选高质量CUDA内核的流程，构建了ConCuR数据集和KernelCoder模型，在KernelBench测试中显著优于现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 解决GPU内核生成中高质量数据稀缺的问题，因为大多数高质量内核是专有且不开源的，阻碍了使用监督微调来对齐LLM进行内核生成任务。

Method: 开发了一个生成和筛选带有推理轨迹的高质量CUDA内核的流程，构建ConCuR数据集，并训练KernelCoder模型。

Result: 在KernelBench测试中，模型显著优于现有最佳模型QwQ-32B，超越所有开源内核生成微调模型以及前沿模型如DeepSeek-V3.1-Think和Claude-4-sonnet。

Conclusion: 平均推理长度可以作为评估内核生成任务难度的指标，该研究的数据收集和筛选流程有助于未来获得更好的内核生成数据。

Abstract: GPU kernel generation by LLMs has recently experienced rapid development,
leveraging test-time scaling and reinforcement learning techniques. However, a
key challenge for kernel generation is the scarcity of high-quality data, as
most high-quality kernels are proprietary and not open-source. This challenge
prevents us from leveraging supervised fine-tuning to align LLMs to the kernel
generation task. To address this challenge, we develop a pipeline that
generates and curates high-quality CUDA kernels with reasoning traces,
motivated by a critical observation that concise yet informative reasoning
traces result in robust generation of high-performance kernels. Using this
pipeline, we construct our dataset ConCuR and introduce our model KernelCoder,
which is the first model trained on a curated dataset consisting of PyTorch,
reasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,
our model achieves significant improvements over the existing top-performing
model, QwQ-32B, and outperforms all open-source models fine-tuned for kernel
generation, as well as frontier models such as DeepSeek-V3.1-Think and
Claude-4-sonnet. Finally, we show that the average reasoning length can serve
as a metric to assess the difficulty of kernel generation tasks. The
observations, metrics, and our data collection and curation pipeline can help
obtain better data in the kernel generation task in the future.

</details>


### [12] [Best-of-Both Worlds for linear contextual bandits with paid observations](https://arxiv.org/abs/2510.07424)
*Nathan Boyer,Dorian Baudry,Patrick Rebeschini*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the problem of linear contextual bandits with paid observations,
where at each round the learner selects an action in order to minimize its loss
in a given context, and can then decide to pay a fixed cost to observe the loss
of any arm. Building on the Follow-the-Regularized-Leader framework with
efficient estimators via Matrix Geometric Resampling, we introduce a
computationally efficient Best-of-Both-Worlds (BOBW) algorithm for this
problem. We show that it achieves the minimax-optimal regret of
$\Theta(T^{2/3})$ in adversarial settings, while guaranteeing poly-logarithmic
regret in (corrupted) stochastic regimes. Our approach builds on the framework
from \cite{BOBWhardproblems} to design BOBW algorithms for ``hard problem'',
using analysis techniques tailored for the setting that we consider.

</details>


### [13] [Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs](https://arxiv.org/abs/2510.07429)
*Wang Wei,Tiankai Yang,Hongjie Chen,Yue Zhao,Franck Dernoncourt,Ryan A. Rossi,Hoda Eldardiry*

Main category: cs.LG

TL;DR: BaRP是一种基于偏好和上下文多臂老虎机的LLM路由方法，在训练时模拟在线反馈环境，支持在推理时动态调整性能/成本权衡，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决LLM部署中的模型选择问题：传统离线训练的路由器假设可以获得所有候选模型的标签，但在实际部署中只能观察到所选模型的结果，存在反馈不完整的问题。

Method: 将LLM路由问题建模为基于提示特征和用户偏好向量的上下文多臂老虎机问题，在训练时模拟在线反馈环境，使路由器适应每个新提示而非依赖完整的离线监督信息。

Result: 实验表明该方法持续优于强离线路由器至少12.46%，优于最大LLM至少2.45%，并且对未见任务具有鲁棒泛化能力。

Conclusion: BaRP方法通过在训练时模拟部署环境的反馈限制，实现了更有效的LLM路由，支持在线决策和灵活的偏好调整，显著提升了路由性能。

Abstract: Efficient use of large language models (LLMs) is critical for deployment at
scale: without adaptive routing, systems either overpay for strong models or
risk poor performance from weaker ones. Selecting the right LLM for each query
is fundamentally an online decision problem: models differ in strengths, prices
fluctuate, and users value accuracy and cost differently. Yet most routers are
trained offline with labels for all candidate models, an assumption that breaks
in deployment, where only the outcome of the chosen model is observed. We
bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach
that trains under the same partial-feedback restriction as deployment, while
supporting preference-tunable inference: operators can dial the
performance/cost trade-off at test time without retraining. Framed as a
contextual bandit over prompt features and a user preference vector, our method
simulates an online feedback setting during training and adapts its routing
decisions to each new prompt, rather than depending on full-information offline
supervision. Comprehensive experiments show that our method consistently
outperforms strong offline routers by at least 12.46% and the largest LLM by at
least 2.45%, and generalizes robustly for unseen tasks.

</details>


### [14] [Parameter-Free Federated TD Learning with Markov Noise in Heterogeneous Environments](https://arxiv.org/abs/2510.07436)
*Ankur Naskar,Gugan Thoppe,Utsav Negi,Vijay Gupta*

Main category: cs.LG

TL;DR: 提出了一种两时间尺度的联邦时序差分学习算法，通过Polyak-Ruppert平均实现了在马尔可夫数据下的最优收敛速率，解决了现有方法依赖未知参数的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习可以加速强化学习，但现有方法在处理马尔可夫链数据时需要依赖未知问题参数，这限制了实际应用。需要开发参数无关的联邦时序差分学习方法。

Method: 采用两时间尺度的联邦时序差分学习框架，结合Polyak-Ruppert平均技术，设计了一种参数无关的算法。

Result: 理论证明该算法在平均奖励和折扣设置下都能达到最优的收敛速率O~(1/NT)，其中N是智能体数量，T是迭代次数。

Conclusion: 该方法首次实现了在马尔可夫数据下参数无关的联邦时序差分学习，即使在异构环境中也能保持最优性能，为联邦强化学习提供了实用的解决方案。

Abstract: Federated learning (FL) can dramatically speed up reinforcement learning by
distributing exploration and training across multiple agents. It can guarantee
an optimal convergence rate that scales linearly in the number of agents, i.e.,
a rate of $\tilde{O}(1/(NT)),$ where $T$ is the iteration index and $N$ is the
number of agents. However, when the training samples arise from a Markov chain,
existing results on TD learning achieving this rate require the algorithm to
depend on unknown problem parameters. We close this gap by proposing a
two-timescale Federated Temporal Difference (FTD) learning with Polyak-Ruppert
averaging. Our method provably attains the optimal $\tilde{O}(1/NT)$ rate in
both average-reward and discounted settings--offering a parameter-free FTD
approach for Markovian data. Although our results are novel even in the
single-agent setting, they apply to the more realistic and challenging scenario
of FL with heterogeneous environments.

</details>


### [15] [HySim-LLM: Embedding-Weighted Fine-Tuning Bounds and Manifold Denoising for Domain-Adapted LLMs](https://arxiv.org/abs/2510.07796)
*Majid Jaberi-Douraki,Hossein Sholehrasa,Xuan Xu,Remya Ampadi Ramachandran*

Main category: cs.LG

TL;DR: 提出HySim-LLM框架，通过嵌入加权微调和流形感知去噪增强LLMs在生物医学结构化数据中的鲁棒性和可解释性，解决药代动力学信息提取的挑战。


<details>
  <summary>Details</summary>
Motivation: 科学文献中药代动力学信息的提取和标准化存在显著挑战，限制了药物开发中数据驱动模型的可靠性。LLMs在文本理解方面取得进展，但在处理生物医学结构化数据时受到异质性、噪声和领域偏移的限制。

Method: 提出HySim-LLM统一数学和计算框架，整合嵌入加权微调和流形感知去噪。建立两个理论结果：(1)基于相似性的泛化边界量化嵌入差异下的适应性能；(2)基于流形的去噪保证限制噪声或离流样本的损失贡献。

Result: 为生物医学结构化环境中微调LLMs提供了原则性理论基础，建立了数学上严谨的泛化边界和去噪保证。

Conclusion: 该框架为生物医学和数据密集型科学领域提供了可靠且可解释的LLM适应的数学基础路径。

Abstract: The extraction and standardization of pharmacokinetic (PK) information from
scientific literature remain significant challenges in computational
pharmacology, which limits the reliability of data-driven models in drug
development. Large language models (LLMs) have achieved remarkable progress in
text understanding and reasoning, yet their adaptation to structured biomedical
data, such as PK tables, remains constrained by heterogeneity, noise, and
domain shift. To address these limitations, we propose HySim-LLM, a unified
mathematical and computational framework that integrates embedding-weighted
fine-tuning and manifold-aware denoising to enhance the robustness and
interpretability of LLMs. We establish two theoretical results: (1) a
similarity-weighted generalization bound that quantifies adaptation performance
under embedding divergence, and (2) a manifold-based denoising guarantee that
bounds loss contributions from noisy or off-manifold samples. These theorems
provide a principled foundation for fine-tuning LLMs in structured biomedical
settings. The framework offers a mathematically grounded pathway toward
reliable and interpretable LLM adaptation for biomedical and data-intensive
scientific domains.

</details>


### [16] [Surrogate Modeling for the Design of Optimal Lattice Structures using Tensor Completion](https://arxiv.org/abs/2510.07474)
*Shaan Pakala,Aldair E. Gongora,Brian Giera,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 该论文提出使用张量补全作为替代模型来加速材料设计，特别是在训练数据来自设计空间非均匀随机采样的场景下。研究表明，在偏置采样情况下，张量补全比传统机器学习方法（如高斯过程和XGBoost）表现更优，R²提高了约5%。


<details>
  <summary>Details</summary>
Motivation: 材料设计中需要寻找具有特定性能的材料，但随着设计变量的增加，搜索空间呈指数级增长，使得合成和验证每个材料的性能变得非常不切实际和耗时。特别是在训练数据来自非均匀随机采样时，传统机器学习方法表现不佳。

Method: 提出使用张量补全作为替代模型来加速材料设计，特别针对训练数据来自设计空间非均匀随机采样的场景。该方法与经典机器学习方法（如高斯过程和XGBoost）进行了比较。

Result: 实验表明，在偏置采样情况下，张量补全比传统机器学习方法表现更优，R²提高了约5%。即使在均匀随机采样整个搜索空间的情况下，张量补全仍能提供相当的性能。

Conclusion: 张量补全是一种有效的替代模型，特别适用于训练数据来自非均匀随机采样的材料设计场景，能够显著加速材料设计过程。

Abstract: When designing new materials, it is often necessary to design a material with
specific desired properties. Unfortunately, as new design variables are added,
the search space grows exponentially, which makes synthesizing and validating
the properties of each material very impractical and time-consuming. In this
work, we focus on the design of optimal lattice structures with regard to
mechanical performance. Computational approaches, including the use of machine
learning (ML) methods, have shown improved success in accelerating materials
design. However, these ML methods are still lacking in scenarios when training
data (i.e. experimentally validated materials) come from a non-uniformly random
sampling across the design space. For example, an experimentalist might
synthesize and validate certain materials more frequently because of
convenience. For this reason, we suggest the use of tensor completion as a
surrogate model to accelerate the design of materials in these atypical
supervised learning scenarios. In our experiments, we show that tensor
completion is superior to classic ML methods such as Gaussian Process and
XGBoost with biased sampling of the search space, with around 5\% increased
$R^2$. Furthermore, tensor completion still gives comparable performance with a
uniformly random sampling of the entire search space.

</details>


### [17] [HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data](https://arxiv.org/abs/2510.07477)
*Maria Mahbub,Robert J. Klein,Myvizhi Esai Selvan,Rowena Yip,Claudia Henschke,Providencia Morales,Ian Goethert,Olivera Kotevska,Mayanka Chandra Shekar,Sean R. Wilkinson,Eileen McAllister,Samuel M. Aguayo,Zeynep H. Gümüş,Ioana Danciu,VA Million Veteran Program*

Main category: cs.LG

TL;DR: HEMERA是一个基于可解释Transformer的深度学习框架，直接处理GWAS原始基因型数据预测肺癌风险，无需临床协变量，在27,254名参与者数据上达到>99% AUC，并能通过后验可解释性模块将预测归因于特定SNP。


<details>
  <summary>Details</summary>
Motivation: 肺癌是美国第三常见癌症和主要癌症死因，虽然吸烟是主要风险因素，但不吸烟者和家族聚集研究显示遗传因素的重要性。GWAS识别的遗传生物标志物是评估肺癌风险的有前景工具。

Method: HEMERA框架应用可解释Transformer深度学习处理GWAS SNP数据，引入加性位置编码、神经基因型嵌入和改进的变异过滤，直接处理原始基因型数据而无需临床协变量。

Result: 在27,254名百万退伍军人计划参与者数据上训练，HEMERA达到>99% AUC得分，后验可解释性模块基于分层集成梯度将模型预测归因于特定SNP，与已知肺癌风险位点高度一致。

Conclusion: 这些发现支持使用透明、假设生成模型进行个性化肺癌风险评估和早期干预。

Abstract: Lung cancer (LC) is the third most common cancer and the leading cause of
cancer deaths in the US. Although smoking is the primary risk factor, the
occurrence of LC in never-smokers and familial aggregation studies highlight a
genetic component. Genetic biomarkers identified through genome-wide
association studies (GWAS) are promising tools for assessing LC risk. We
introduce HEMERA (Human-Explainable Transformer Model for Estimating Lung
Cancer Risk using GWAS Data), a new framework that applies explainable
transformer-based deep learning to GWAS data of single nucleotide polymorphisms
(SNPs) for predicting LC risk. Unlike prior approaches, HEMERA directly
processes raw genotype data without clinical covariates, introducing additive
positional encodings, neural genotype embeddings, and refined variant
filtering. A post hoc explainability module based on Layer-wise Integrated
Gradients enables attribution of model predictions to specific SNPs, aligning
strongly with known LC risk loci. Trained on data from 27,254 Million Veteran
Program participants, HEMERA achieved >99% AUC (area under receiver
characteristics) score. These findings support transparent,
hypothesis-generating models for personalized LC risk assessment and early
intervention.

</details>


### [18] [PEAR: Planner-Executor Agent Robustness Benchmark](https://arxiv.org/abs/2510.07505)
*Shen Dong,Mingxuan Zhang,Pengfei He,Li Ma,Bhavani Thuraisingham,Hui Liu,Yue Xing*

Main category: cs.LG

TL;DR: PEAR是一个用于系统评估规划器-执行器多智能体系统效用和脆弱性的基准测试，发现规划器比执行器更关键，存在任务性能与鲁棒性的权衡，针对规划器的攻击特别有效。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大语言模型的多智能体系统在处理复杂任务方面表现出强大能力，但它们容易受到对抗性操纵。现有研究通常只关注孤立的攻击面或特定场景，缺乏对多智能体系统漏洞的整体理解。

Method: 引入PEAR基准测试，专注于规划器-执行器结构，通过广泛实验系统评估多智能体系统的效用和脆弱性。

Result: 实验发现：(1)弱规划器比弱执行器更严重影响整体任务性能；(2)规划器需要内存模块，但执行器的内存模块不影响任务性能；(3)任务性能与鲁棒性存在权衡；(4)针对规划器的攻击特别有效。

Conclusion: 这些发现为增强多智能体系统的鲁棒性提供了可行见解，并为多智能体环境中的原则性防御奠定了基础。

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a
powerful paradigm for tackling complex, multi-step tasks across diverse
domains. However, despite their impressive capabilities, MAS remain susceptible
to adversarial manipulation. Existing studies typically examine isolated attack
surfaces or specific scenarios, leaving a lack of holistic understanding of MAS
vulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for
systematically evaluating both the utility and vulnerability of
planner-executor MAS. While compatible with various MAS architectures, our
benchmark focuses on the planner-executor structure, which is a practical and
widely adopted design. Through extensive experiments, we find that (1) a weak
planner degrades overall clean task performance more severely than a weak
executor; (2) while a memory module is essential for the planner, having a
memory module for the executor does not impact the clean task performance; (3)
there exists a trade-off between task performance and robustness; and (4)
attacks targeting the planner are particularly effective at misleading the
system. These findings offer actionable insights for enhancing the robustness
of MAS and lay the groundwork for principled defenses in multi-agent settings.

</details>


### [19] [Efficient Generalization via Multimodal Co-Training under Data Scarcity and Distribution Shift](https://arxiv.org/abs/2510.07509)
*Tianyu Bell Pan,Damon L. Woodard*

Main category: cs.LG

TL;DR: 本文提出了一种多模态协同训练框架，旨在解决标注数据有限和分布偏移情况下的模型泛化问题，通过理论分析和收敛性证明验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中，标注数据通常有限且数据分布会发生变化，这给AI系统的泛化能力带来挑战。多模态协同训练通过利用未标注数据和促进不同模态分类器之间的一致性，有望提升模型在动态环境中的泛化性能。

Method: 采用多模态协同训练框架，结合理论分析推导了使用未标注数据和促进模态间分类器一致性的条件，进行了收敛性分析，并建立了新的泛化边界来量化多模态协同训练的优势。

Result: 理论分析表明，在特定条件下，利用未标注多模态数据和促进模态间一致性可以显著改善泛化性能。收敛性分析证实了迭代协同训练在减少分类错误方面的有效性。

Conclusion: 多模态协同训练为开发数据高效且鲁棒的AI系统提供了结构化方法，能够在动态现实环境中有效泛化，理论分析为该方法提供了坚实的理论基础。

Abstract: This paper explores a multimodal co-training framework designed to enhance
model generalization in situations where labeled data is limited and
distribution shifts occur. We thoroughly examine the theoretical foundations of
this framework, deriving conditions under which the use of unlabeled data and
the promotion of agreement between classifiers for different modalities lead to
significant improvements in generalization. We also present a convergence
analysis that confirms the effectiveness of iterative co-training in reducing
classification errors. In addition, we establish a novel generalization bound
that, for the first time in a multimodal co-training context, decomposes and
quantifies the distinct advantages gained from leveraging unlabeled multimodal
data, promoting inter-view agreement, and maintaining conditional view
independence. Our findings highlight the practical benefits of multimodal
co-training as a structured approach to developing data-efficient and robust AI
systems that can effectively generalize in dynamic, real-world environments.
The theoretical foundations are examined in dialogue with, and in advance of,
established co-training principles.

</details>


### [20] [MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis](https://arxiv.org/abs/2510.07513)
*Qinghua Liu,Sam Heshmati,Zheda Mai,Zubin Abraham,John Paparrizos,Liu Ren*

Main category: cs.LG

TL;DR: MLLM4TS是一个利用多模态大语言模型进行时间序列分析的框架，通过将时间序列数据转换为视觉表示来弥补数值数据与自然语言之间的模态差距。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析面临复杂时间依赖性和跨通道交互的挑战，受人类视觉检查时间序列模式的启发，研究如何通过视觉表示增强自动化时间序列分析。

Method: 将每个时间序列通道渲染为水平堆叠的彩色编码线图，使用时间感知视觉补丁对齐策略将视觉补丁与对应时间段对齐，融合数值数据的细粒度时间细节和视觉表示的全局上下文信息。

Result: 在标准基准测试上的广泛实验表明，MLLM4TS在预测任务（如分类）和生成任务（如异常检测和预测）中都表现出有效性。

Conclusion: 将视觉模态与预训练语言模型相结合具有实现稳健和可泛化时间序列分析的潜力。

Abstract: Effective analysis of time series data presents significant challenges due to
the complex temporal dependencies and cross-channel interactions in
multivariate data. Inspired by the way human analysts visually inspect time
series to uncover hidden patterns, we ask: can incorporating visual
representations enhance automated time-series analysis? Recent advances in
multimodal large language models have demonstrated impressive generalization
and visual understanding capability, yet their application to time series
remains constrained by the modality gap between continuous numerical data and
discrete natural language. To bridge this gap, we introduce MLLM4TS, a novel
framework that leverages multimodal large language models for general
time-series analysis by integrating a dedicated vision branch. Each time-series
channel is rendered as a horizontally stacked color-coded line plot in one
composite image to capture spatial dependencies across channels, and a
temporal-aware visual patch alignment strategy then aligns visual patches with
their corresponding time segments. MLLM4TS fuses fine-grained temporal details
from the numerical data with global contextual information derived from the
visual representation, providing a unified foundation for multimodal
time-series analysis. Extensive experiments on standard benchmarks demonstrate
the effectiveness of MLLM4TS across both predictive tasks (e.g.,
classification) and generative tasks (e.g., anomaly detection and forecasting).
These results underscore the potential of integrating visual modalities with
pretrained language models to achieve robust and generalizable time-series
analysis.

</details>


### [21] [EEG Sleep Stage Classification with Continuous Wavelet Transform and Deep Learning](https://arxiv.org/abs/2510.07524)
*Mehdi Zekriyapanah Gashti,Ghasem Farjamnia*

Main category: cs.LG

TL;DR: 提出了一种基于小波变换的自动睡眠分期新框架，通过连续小波变换生成时频图捕捉睡眠相关的瞬态和振荡模式，结合集成学习在Sleep-EDF数据库上达到88.37%的准确率。


<details>
  <summary>Details</summary>
Motivation: 准确的睡眠分期对于睡眠障碍的诊断和管理至关重要，传统方法依赖人工标注或从EEG信号中提取时域/频域特征，需要更自动化和准确的方法。

Method: 使用连续小波变换生成时频图，捕捉与睡眠分期相关的频带中的瞬态和振荡模式，然后结合集成学习方法进行分类。

Result: 在Sleep-EDF扩展数据库上，该方法达到88.37%的整体准确率和73.15的宏平均F1分数，优于传统机器学习方法，与最近的深度学习方法性能相当或更优。

Conclusion: 小波分析在睡眠分期分类中具有稳健、可解释和临床应用的潜力。

Abstract: Accurate classification of sleep stages is crucial for the diagnosis and
management of sleep disorders. Conventional approaches for sleep scoring rely
on manual annotation or features extracted from EEG signals in the time or
frequency domain. This study proposes a novel framework for automated sleep
stage scoring using time-frequency analysis based on the wavelet transform. The
Sleep-EDF Expanded Database (sleep-cassette recordings) was used for
evaluation. The continuous wavelet transform (CWT) generated time-frequency
maps that capture both transient and oscillatory patterns across frequency
bands relevant to sleep staging. Experimental results demonstrate that the
proposed wavelet-based representation, combined with ensemble learning,
achieves an overall accuracy of 88.37 percent and a macro-averaged F1 score of
73.15, outperforming conventional machine learning methods and exhibiting
comparable or superior performance to recent deep learning approaches. These
findings highlight the potential of wavelet analysis for robust, interpretable,
and clinically applicable sleep stage classification.

</details>


### [22] [Estimating Fair Graphs from Graph-Stationary Data](https://arxiv.org/abs/2510.07536)
*Madeline Navarro,Andrei Buciulea,Samuel Rey,Antonio G. Marques,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出FairSpecTemp方法从图平稳节点观测中估计公平图，避免连接对敏感属性的偏见。提供多种偏见度量指标，包括谱域新测量方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界图中的边往往表现出对某些群体对的连接偏好，这种有偏连接会加剧甚至引发下游图任务的歧视性处理。

Method: 提出FairSpecTemp优化方法，有两种变体：一种利用图平稳性的交换性直接约束偏见，另一种通过限制图谱中的偏见间接鼓励公平估计。

Result: 方法具有高概率性能边界，在公平性和准确性之间形成条件权衡。分析表明恢复公平图无需牺牲准确性。在合成和真实数据集上验证有效性。

Conclusion: FairSpecTemp能有效估计公平图，两种变体各有优势，为图公平性研究提供了新工具。

Abstract: We estimate fair graphs from graph-stationary nodal observations such that
connections are not biased with respect to sensitive attributes. Edges in
real-world graphs often exhibit preferences for connecting certain pairs of
groups. Biased connections can not only exacerbate but even induce unfair
treatment for downstream graph-based tasks. We therefore consider group and
individual fairness for graphs corresponding to group- and node-level
definitions, respectively. To evaluate the fairness of a given graph, we
provide multiple bias metrics, including novel measurements in the spectral
domain. Furthermore, we propose Fair Spectral Templates (FairSpecTemp), an
optimization-based method with two variants for estimating fair graphs from
stationary graph signals, a general model for graph data subsuming many
existing ones. One variant of FairSpecTemp exploits commutativity properties of
graph stationarity while directly constraining bias, while the other implicitly
encourages fair estimates by restricting bias in the graph spectrum and is thus
more flexible. Our methods enjoy high probability performance bounds, yielding
a conditional tradeoff between fairness and accuracy. In particular, our
analysis reveals that accuracy need not be sacrificed to recover fair graphs.
We evaluate FairSpecTemp on synthetic and real-world data sets to illustrate
its effectiveness and highlight the advantages of both variants of
FairSpecTemp.

</details>


### [23] [Targeted Digital Twin via Flow Map Learning and Its Application to Fluid Dynamics](https://arxiv.org/abs/2510.07549)
*Qifan Chen,Zhongshu Xu,Jinjin Zhang,Dongbin Xiu*

Main category: cs.LG

TL;DR: 提出了一种构建目标数字孪生(tDT)的数值框架，通过基于记忆的流映射学习(FML)直接从全数字孪生中建模感兴趣量的动态，实现离线计算和在线高效预测。


<details>
  <summary>Details</summary>
Motivation: 为了解决全数字孪生模拟计算成本高的问题，需要开发能够直接建模感兴趣量动态的轻量化方法，实现计算效率的提升。

Method: 采用基于记忆的流映射学习(FML)方法，利用全数字孪生生成的短轨迹数据构建数据驱动模型，形成完全离线的计算过程。

Result: 在二维不可压缩圆柱绕流CFD案例中，成功构建了能够准确预测长期流体动力力的紧凑动态系统，完全绕过了全流场模拟。

Conclusion: 该方法能够显著节省计算成本，为目标数字孪生的构建提供了有效的数值框架，在保持预测精度的同时实现了计算效率的优化。

Abstract: We present a numerical framework for constructing a targeted digital twin
(tDT) that directly models the dynamics of quantities of interest (QoIs) in a
full digital twin (DT). The proposed approach employs memory-based flow map
learning (FML) to develop a data-driven model of the QoIs using short bursts of
trajectory data generated through repeated executions of the full DT. This
renders the construction of the FML-based tDT an entirely offline computational
process. During online simulation, the learned tDT can efficiently predict and
analyze the long-term dynamics of the QoIs without requiring simulations of the
full DT system, thereby achieving substantial computational savings. After
introducing the general numerical procedure, we demonstrate the construction
and predictive capability of the tDT in a computational fluid dynamics (CFD)
example: two-dimensional incompressible flow past a cylinder. The QoIs in this
problem are the hydrodynamic forces exerted on the cylinder. The resulting tDTs
are compact dynamical systems that evolve these forces without explicit
knowledge of the underlying flow field. Numerical results show that the tDTs
yield accurate long-term predictions of the forces while entirely bypassing
full flow simulations.

</details>


### [24] [Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic](https://arxiv.org/abs/2510.07557)
*Abhay Bhandarkar,Gaurav Mishra,Khushi Juchani,Harsh Singhal*

Main category: cs.LG

TL;DR: 本研究使用BERTopic主题建模技术分析lmsys-chat-1m多语言对话数据集，旨在发现对话中的主题模式及其与用户偏好的关系，探索特定LLM是否在特定主题中更受青睐。


<details>
  <summary>Details</summary>
Motivation: 分析多语言对话语料库中的主题模式，研究这些主题与用户对大型语言模型输出偏好的关系，特别是确定某些LLM是否在特定主题中持续获得用户偏好。

Method: 设计了针对多语言变体的鲁棒预处理流程，平衡对话轮次并清理噪声数据；应用BERTopic提取主题；使用主题间距离图、主题概率分布和模型-主题矩阵等可视化技术分析主题与模型偏好的关系。

Result: BERTopic提取了29个连贯主题，包括人工智能、编程、伦理和云基础设施等；分析了主题与模型偏好之间的关系，识别了模型-主题对齐的趋势。

Conclusion: 研究结果为领域特定微调和优化策略提供了信息，有助于改进现实世界中LLM的性能和用户满意度。

Abstract: This study applies BERTopic, a transformer-based topic modeling technique, to
the lmsys-chat-1m dataset, a multilingual conversational corpus built from
head-to-head evaluations of large language models (LLMs). Each user prompt is
paired with two anonymized LLM responses and a human preference label, used to
assess user evaluation of competing model outputs. The main objective is
uncovering thematic patterns in these conversations and examining their
relation to user preferences, particularly if certain LLMs are consistently
preferred within specific topics. A robust preprocessing pipeline was designed
for multilingual variation, balancing dialogue turns, and cleaning noisy or
redacted data. BERTopic extracted over 29 coherent topics including artificial
intelligence, programming, ethics, and cloud infrastructure. We analysed
relationships between topics and model preferences to identify trends in
model-topic alignment. Visualization techniques included inter-topic distance
maps, topic probability distributions, and model-versus-topic matrices. Our
findings inform domain-specific fine-tuning and optimization strategies for
improving real-world LLM performance and user satisfaction.

</details>


### [25] [Automated Machine Learning for Unsupervised Tabular Tasks](https://arxiv.org/abs/2510.07569)
*Prabhant Singh,Pieter Gijsbers,Elif Ceren Gok Yildirim,Murat Onur Yildirim,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: LOTUS是一个用于无监督机器学习任务（如异常检测和聚类）的模型选择方法，通过最优传输距离来度量未标记表格数据集之间的相似性，并推荐在相似数据集上表现良好的机器学习流程。


<details>
  <summary>Details</summary>
Motivation: 研究动机是基于一个直觉：如果一个机器学习流程在具有相似底层数据分布的数据集上表现良好，那么它很可能在新的数据集上也会表现良好。

Method: 使用最优传输距离来度量未标记表格数据集之间的相似性，并基于这种相似性推荐机器学习流程。该方法统一适用于异常检测和聚类两种无监督任务。

Result: 实验结果表明，LOTUS在异常检测和聚类任务上表现优于强基线方法，显示出作为多无监督机器学习任务模型选择方法的潜力。

Conclusion: LOTUS是朝着多无监督机器学习任务模型选择方向迈出的非常有前景的第一步，其简单而有效的方法在实验中表现良好。

Abstract: In this work, we present LOTUS (Learning to Learn with Optimal Transport for
Unsupervised Scenarios), a simple yet effective method to perform model
selection for multiple unsupervised machine learning(ML) tasks such as outlier
detection and clustering. Our intuition behind this work is that a machine
learning pipeline will perform well in a new dataset if it previously worked
well on datasets with a similar underlying data distribution. We use Optimal
Transport distances to find this similarity between unlabeled tabular datasets
and recommend machine learning pipelines with one unified single method on two
downstream unsupervised tasks: outlier detection and clustering. We present the
effectiveness of our approach with experiments against strong baselines and
show that LOTUS is a very promising first step toward model selection for
multiple unsupervised ML tasks.

</details>


### [26] [Expanding the Action Space of LLMs to Reason Beyond Language](https://arxiv.org/abs/2510.07581)
*Zhongqi Yue,Weishi Wang,Yundaichuan Zhan,Juncheng Li,Daniel Dahlmeier,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 该论文提出ExpA（扩展动作空间）方法，将环境交互从语言中解耦，允许LLM在语言环境和外部环境之间切换，并通过EARL强化学习优化策略，在需要多轮交互和条件规划的任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的环境交互必须通过预定义格式的文本表达，需要手动解析器，这使语言模型同时承担推理和控制职责，限制了其与外部环境的有效交互。

Method: 提出ExpA方法，将环境交互内部化到词汇表之外的扩展动作空间；引入EARL强化学习，使用反事实策略优化来促进对新环境和扩展动作空间的有效探索。

Result: 在需要多轮交互和条件规划的任务中，EARL优于词汇约束动作的强基线方法；在基于计算器的多任务学习中表现稳健；在部分可观测排序问题中实现100%的Sort-4准确率，并自发现与经典设计相竞争的高效算法。

Conclusion: 通过将环境交互从语言中解耦并内部化到扩展动作空间，结合EARL强化学习，可以显著提升LLM在复杂环境交互任务中的性能，实现更高效的多轮交互和条件规划。

Abstract: Large Language Models (LLMs) are powerful reasoners in natural language, but
their actions are typically confined to outputting vocabulary tokens. As a
result, interactions with external environments -- such as symbolic operators
or simulators -- must be expressed through text in predefined formats, parsed,
and routed to external interfaces. This overloads the model's language with
both reasoning and control duties, and requires a hand-crafted parser, external
to the LLM. To address this, we decouple environment interactions from language
by internalizing them in an Expanded Action space (ExpA), beyond the
vocabulary. The model starts reasoning in the default language environment, but
may trigger routing actions and switch to an external environment at any time.
From there, the model can only invoke environment-specific actions, receive
feedback from the environment, and potentially route back to language as a
result. To promote effective exploration of the expanded action space and new
environments, we introduce ExpA Reinforcement Learning (EARL) with
counterfactual policy optimization. On tasks requiring multi-turn interactions
and contingent planning, EARL outperforms strong baselines with
vocabulary-constrained actions. It performs robustly across calculator-based
multi-task learning and, in the partially observed sorting problem, achieves
perfect Sort-4 accuracy while self-discovering an efficient algorithm
competitive with classical designs.

</details>


### [27] [TGM: a Modular and Efficient Library for Machine Learning on Temporal Graphs](https://arxiv.org/abs/2510.07586)
*Jacob Chmura,Shenyang Huang,Tran Gia Bao Ngo,Ali Parviz,Farimah Poursafaei,Jure Leskovec,Michael Bronstein,Guillaume Rabusseau,Matthias Fey,Reihaneh Rabbany*

Main category: cs.LG

TL;DR: TGM是一个研究导向的时序图机器学习库，首次统一了连续时间和离散时间动态图方法，提供动态节点特征支持、时间粒度转换和原生处理链接/节点/图级任务，相比现有库实现了显著加速并解锁了新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前静态图机器学习有成熟的框架，但时序图ML缺乏可比的基础设施。现有时序图库通常针对特定架构设计，难以支持这个快速发展领域的多样化模型，且连续时间和离散时间动态图方法之间的鸿沟限制了直接比较和思想交流。

Method: 开发TGM时序图建模库，统一连续时间动态图(CTDG)和离散时间动态图(DTDG)方法，提供动态节点特征的一流支持、时间粒度转换功能，以及链接级、节点级和图级任务的原生处理能力。

Result: TGM在多个模型、数据集和任务上相比广泛使用的DyGLib实现了平均7.8倍加速，在图离散化方面相比可用实现实现了平均175倍加速。实验表明TGM能够实现动态图属性预测和时间驱动训练范式，解锁了以前难以研究的新研究方向。

Conclusion: TGM填补了时序图机器学习基础设施的空白，通过统一连续时间和离散时间方法、提供高效实现和灵活功能，不仅显著提升了性能，还为时序图研究开辟了新的可能性。

Abstract: Well-designed open-source software drives progress in Machine Learning (ML)
research. While static graph ML enjoys mature frameworks like PyTorch Geometric
and DGL, ML for temporal graphs (TG), networks that evolve over time, lacks
comparable infrastructure. Existing TG libraries are often tailored to specific
architectures, hindering support for diverse models in this rapidly evolving
field. Additionally, the divide between continuous- and discrete-time dynamic
graph methods (CTDG and DTDG) limits direct comparisons and idea transfer. To
address these gaps, we introduce Temporal Graph Modelling (TGM), a
research-oriented library for ML on temporal graphs, the first to unify CTDG
and DTDG approaches. TGM offers first-class support for dynamic node features,
time-granularity conversions, and native handling of link-, node-, and
graph-level tasks. Empirically, TGM achieves an average 7.8x speedup across
multiple models, datasets, and tasks compared to the widely used DyGLib, and an
average 175x speedup on graph discretization relative to available
implementations. Beyond efficiency, we show in our experiments how TGM unlocks
entirely new research possibilities by enabling dynamic graph property
prediction and time-driven training paradigms, opening the door to questions
previously impractical to study. TGM is available at
https://github.com/tgm-team/tgm

</details>


### [28] [Transformer-Based Indirect Structural Health Monitoring of Rail Infrastructure with Attention-Driven Detection and Localization of Transient Defects](https://arxiv.org/abs/2510.07606)
*Sizhe Ma,Katherine A. Flanigan,Mario Bergés,James D. Brooks*

Main category: cs.LG

TL;DR: 本文提出了一种基于注意力机制的Transformer模型，用于铁路轨道间接结构健康监测中的断轨检测，通过无监督学习方法解决小尺寸瞬态异常检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基于车载传感器的间接结构健康监测方法在检测小尺寸（2-10厘米）瞬态异常时面临挑战，主要由于复杂的车辆动力学、信号噪声以及标记数据稀缺限制了监督学习方法的应用。

Method: 提出了一种注意力聚焦Transformer模型，采用自注意力机制，通过重构训练但创新性地主要从学习到的注意力权重偏差中推导异常分数，旨在实现有效性和计算效率。

Result: 基准测试结果表明，Transformer模型普遍优于其他模型，但所有测试模型都对高频局部噪声表现出显著脆弱性。提出的模型在达到与最先进解决方案相当的准确性的同时，展现出更好的推理速度。

Conclusion: 研究强调了未来iSHM模型增强噪声鲁棒性的关键需求，并将更高效的基于注意力的方法定位为开发实用车载异常检测系统的有前景基础。

Abstract: Indirect structural health monitoring (iSHM) for broken rail detection using
onboard sensors presents a cost-effective paradigm for railway track
assessment, yet reliably detecting small, transient anomalies (2-10 cm) remains
a significant challenge due to complex vehicle dynamics, signal noise, and the
scarcity of labeled data limiting supervised approaches. This study addresses
these issues through unsupervised deep learning. We introduce an incremental
synthetic data benchmark designed to systematically evaluate model robustness
against progressively complex challenges like speed variations, multi-channel
inputs, and realistic noise patterns encountered in iSHM. Using this benchmark,
we evaluate several established unsupervised models alongside our proposed
Attention-Focused Transformer. Our model employs a self-attention mechanism,
trained via reconstruction but innovatively deriving anomaly scores primarily
from deviations in learned attention weights, aiming for both effectiveness and
computational efficiency. Benchmarking results reveal that while
transformer-based models generally outperform others, all tested models exhibit
significant vulnerability to high-frequency localized noise, identifying this
as a critical bottleneck for practical deployment. Notably, our proposed model
achieves accuracy comparable to the state-of-the-art solution while
demonstrating better inference speed. This highlights the crucial need for
enhanced noise robustness in future iSHM models and positions our more
efficient attention-based approach as a promising foundation for developing
practical onboard anomaly detection systems.

</details>


### [29] [LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics](https://arxiv.org/abs/2510.07626)
*Chongyu Fan,Changsheng Wang,Yancheng Huang,Soumyadeep Pal,Sijia Liu*

Main category: cs.LG

TL;DR: 本文对大型语言模型（LLM）的遗忘技术进行了系统性回顾，提出了12种状态遗忘方法的分类法，并重新评估了遗忘效果、效用保持和鲁棒性，指出当前评估方法的局限性并引入了开放式问答指标。


<details>
  <summary>Details</summary>
Motivation: LLM遗忘研究目前缺乏清晰的定义和严格的评估标准，现有评估方法过于依赖选择题准确率，无法全面反映模型的实际生成行为，需要更系统的评估框架。

Method: 提出了三种方法家族的分类法：基于分歧的优化、表征错位和基于拒绝的目标遗忘；引入开放式问答指标来评估生成性能；对WMDP基准进行重新评估。

Result: 发现当前评估方法高估了遗忘成功率，忽视了模型的生成行为；开放式问答指标揭示了遗忘效果与效用保持之间的权衡关系；鲁棒性分析显示不同攻击类型的脆弱性存在显著差异。

Conclusion: 提供了LLM遗忘技术的全面重新审视，为未来方法的设计和评估提供了可操作的指导，强调了需要更全面的评估框架来准确衡量遗忘效果。

Abstract: Machine unlearning for large language models (LLMs) aims to remove undesired
data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while
preserving useful model capabilities. Despite rapid progress over the past two
years, research in LLM unlearning remains fragmented, with limited clarity on
what constitutes effective unlearning and how it should be rigorously
evaluated. In this work, we present a principled taxonomy of twelve recent
stateful unlearning methods, grouped into three methodological families:
divergence-driven optimization, representation misalignment, and
rejection-based targeted unlearning. Building on this taxonomy, we revisit the
evaluation of unlearning effectiveness (UE), utility retention (UT), and
robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that
current evaluations, dominated by multiple-choice question (MCQ) accuracy,
offer only a narrow perspective, often overstating success while overlooking
the model's actual generation behavior. To address this gap, we introduce open
question-answering (Open-QA) metrics that better capture generative performance
and reveal the inherent UE-UT tradeoff across method families. Furthermore, we
demonstrate that robustness requires finer-grained analysis: for example,
vulnerabilities differ substantially between in-domain relearning and
out-of-domain fine-tuning, even though both fall under model-level attacks.
Through this study, we hope to deliver a full-stack revisit of LLM unlearning
and actionable guidance for designing and evaluating future methods.

</details>


### [30] [Property Classification of Vacation Rental Properties during Covid-19](https://arxiv.org/abs/2510.07639)
*Favour Yahdii Aghaebe,Dustin Foley,Eric Atwell,Stephen Clark*

Main category: cs.LG

TL;DR: 本研究使用K-means和K-medoids聚类技术对新冠疫情期间活跃的度假租赁房产进行分类，以识别固有模式和行为特征。


<details>
  <summary>Details</summary>
Motivation: 通过聚类分析来理解度假租赁市场的复杂性和识别房产的固有模式，为制定针对性政策提供依据。

Method: 使用K-means和K-medoids聚类技术，基于包含超过100万房产和房东的数据集进行分析。

Result: 识别出了同质化的房产群体及其共同特征，增强了对度假租赁评估复杂性的理解。

Conclusion: 研究结果可用于制定针对特定聚类的精准政策，提高政策制定的有效性。

Abstract: This study advocates for employing clustering techniques to classify vacation
rental properties active during the Covid pandemic to identify inherent
patterns and behaviours. The dataset, a collaboration between the ESRC funded
Consumer Data Research Centre (CDRC) and AirDNA, encompasses data for over a
million properties and hosts. Utilising K-means and K-medoids clustering
techniques, we identify homogenous groups and their common characteristics. Our
findings enhance comprehension of the intricacies of vacation rental
evaluations and could potentially be utilised in the creation of targeted,
cluster-specific policies.

</details>


### [31] [Value Flows](https://arxiv.org/abs/2510.07650)
*Perry Dong,Chongyi Zheng,Chelsea Finn,Dorsa Sadigh,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文提出Value Flows方法，使用基于流的模型估计完整的未来回报分布，通过流匹配目标生成满足分布贝尔曼方程的概率密度路径，并利用流导数ODE估计状态回报不确定性，在37个状态基准和25个图像基准任务上平均成功率提升1.3倍。


<details>
  <summary>Details</summary>
Motivation: 当前大多数强化学习方法将未来回报分布简化为单个标量值，而分布RL方法利用回报分布提供更强的学习信号，但现有方法使用离散分箱或有限分位数估计，无法揭示回报分布的细粒度结构，也难以区分高回报不确定性的状态进行决策。

Method: 使用现代灵活的基于流模型估计完整未来回报分布，提出新的流匹配目标生成满足分布贝尔曼方程的概率密度路径，基于学习到的流模型使用新的流导数ODE估计状态回报不确定性，并利用不确定性信息优先学习某些转换的更准确回报估计。

Result: 在37个状态基准和25个图像基准任务上的实验表明，Value Flows方法平均成功率提升1.3倍。

Conclusion: 基于流的模型能够有效估计完整回报分布并识别高不确定性状态，为强化学习提供了更强大的分布估计能力。

Abstract: While most reinforcement learning methods today flatten the distribution of
future returns to a single scalar value, distributional RL methods exploit the
return distribution to provide stronger learning signals and to enable
applications in exploration and safe RL. While the predominant method for
estimating the return distribution is by modeling it as a categorical
distribution over discrete bins or estimating a finite number of quantiles,
such approaches leave unanswered questions about the fine-grained structure of
the return distribution and about how to distinguish states with high return
uncertainty for decision-making. The key idea in this paper is to use modern,
flexible flow-based models to estimate the full future return distributions and
identify those states with high return variance. We do so by formulating a new
flow-matching objective that generates probability density paths satisfying the
distributional Bellman equation. Building upon the learned flow models, we
estimate the return uncertainty of distinct states using a new flow derivative
ODE. We additionally use this uncertainty information to prioritize learning a
more accurate return estimation on certain transitions. We compare our method
(Value Flows) with prior methods in the offline and online-to-online settings.
Experiments on $37$ state-based and $25$ image-based benchmark tasks
demonstrate that Value Flows achieves a $1.3\times$ improvement on average in
success rates. Website: https://pd-perry.github.io/value-flows Code:
https://github.com/chongyi-zheng/value-flows

</details>


### [32] [Incremental Hybrid Ensemble with Graph Attention and Frequency-Domain Features for Stable Long-Term Credit Risk Modeling](https://arxiv.org/abs/2510.07663)
*Jiajing Wang*

Main category: cs.LG

TL;DR: HYDRA-EI是一个混合集成增量学习框架，用于预测长期贷款违约，通过多阶段特征处理和模型组合来应对数据分布随时间变化的问题。


<details>
  <summary>Details</summary>
Motivation: 预测长期贷款违约具有挑战性，因为借款人行为经常变化，数据分布会随时间推移而偏移。

Method: 构建关系型、交叉和基于频率的特征，使用图注意力、自动交叉特征创建和频域变换，每周使用新数据更新模型，并通过基于性能的简单方法调整模型权重。

Result: HYDRA-EI提高了模型稳定性和泛化能力。

Conclusion: 该框架适用于长期信用风险任务，无需频繁手动更改或固定重新训练。

Abstract: Predicting long-term loan defaults is hard because borrower behavior often
changes and data distributions shift over time. This paper presents HYDRA-EI, a
hybrid ensemble incremental learning framework. It uses several stages of
feature processing and combines multiple models. The framework builds
relational, cross, and frequency-based features. It uses graph attention,
automatic cross-feature creation, and transformations from the frequency
domain. HYDRA-EI updates weekly using new data and adjusts the model weights
with a simple performance-based method. It works without frequent manual
changes or fixed retraining. HYDRA-EI improves model stability and
generalization, which makes it useful for long-term credit risk tasks.

</details>


### [33] [FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning](https://arxiv.org/abs/2510.07664)
*Yunbo Li,Jiaping Gui,Zhihang Deng,Fanchao Meng,Yue Wu*

Main category: cs.LG

TL;DR: FedQS是一个用于半异步联邦学习（SAFL）的框架，通过客户端分类和自适应优化来解决梯度聚合和模型聚合之间的权衡问题，实现了更高的准确性、更低的损失和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 半异步联邦学习（SAFL）在梯度聚合和模型聚合策略之间存在显著的权衡问题：梯度聚合收敛快但波动大，模型聚合稳定但收敛慢且精度不足。需要一种统一的方法来平衡这些权衡。

Method: FedQS采用分治策略，根据数据分布特征和计算资源将客户端分为四种类型，并自适应优化其本地训练。这是第一个在理论上分析和解决SAFL中聚合策略差异的框架。

Result: 在计算机视觉、自然语言处理和实际任务上的大量实验表明，FedQS实现了最高精度、最低损失，并且在收敛速度方面排名前列，优于现有最先进的基线方法。

Conclusion: FedQS弥合了SAFL中聚合策略之间的差距，为稳定、准确和高效的联邦学习提供了统一解决方案。

Abstract: Federated learning (FL) enables collaborative model training across multiple
parties without sharing raw data, with semi-asynchronous FL (SAFL) emerging as
a balanced approach between synchronous and asynchronous FL. However, SAFL
faces significant challenges in optimizing both gradient-based (e.g., FedSGD)
and model-based (e.g., FedAvg) aggregation strategies, which exhibit distinct
trade-offs in accuracy, convergence speed, and stability. While gradient
aggregation achieves faster convergence and higher accuracy, it suffers from
pronounced fluctuations, whereas model aggregation offers greater stability but
slower convergence and suboptimal accuracy. This paper presents FedQS, the
first framework to theoretically analyze and address these disparities in SAFL.
FedQS introduces a divide-and-conquer strategy to handle client heterogeneity
by classifying clients into four distinct types and adaptively optimizing their
local training based on data distribution characteristics and available
computational resources. Extensive experiments on computer vision, natural
language processing, and real-world tasks demonstrate that FedQS achieves the
highest accuracy, attains the lowest loss, and ranks among the fastest in
convergence speed, outperforming state-of-the-art baselines. Our work bridges
the gap between aggregation strategies in SAFL, offering a unified solution for
stable, accurate, and efficient federated learning. The code and datasets are
available at https://anonymous.4open.science/r/FedQS-EDD6.

</details>


### [34] [Computationally-efficient Graph Modeling with Refined Graph Random Features](https://arxiv.org/abs/2510.07716)
*Krzysztof Choromanski,Avinava Dubey,Arijit Sehanobish,Isaac Reid*

Main category: cs.LG

TL;DR: GRFs++是一种改进的图随机特征方法，通过行走缝合技术和更灵活的行走长度分布策略，解决了传统GRFs在建模远距离节点关系和采样效率方面的限制，实现了更高效准确的图核计算。


<details>
  <summary>Details</summary>
Motivation: 传统图随机特征(GRFs)存在长期局限性，包括难以建模远距离节点关系、依赖采样长随机行走导致效率低下，以及行走终止机制过于简单。GRFs++旨在解决这些问题，提供更高效准确的图核计算方法。

Method: 1. 行走缝合技术：将多个短行走拼接而不破坏无偏性；2. 并行计算短行走和矩阵乘法，替代顺序采样长行走；3. 扩展行走终止机制，应用更一般的行走长度分布策略。

Result: GRFs++继承了长行走提供的近似质量，但具有更高的计算效率。在不增加额外计算成本的情况下，提高了图核的近似精度。通过实证评估验证了所有主张。

Conclusion: GRFs++通过创新的行走缝合技术和灵活的行走长度分布策略，成功解决了传统GRFs的关键限制，为图核计算提供了更高效准确的解决方案，并通过理论分析和实验验证了其有效性。

Abstract: We propose refined GRFs (GRFs++), a new class of Graph Random Features (GRFs)
for efficient and accurate computations involving kernels defined on the nodes
of a graph. GRFs++ resolve some of the long-standing limitations of regular
GRFs, including difficulty modeling relationships between more distant nodes.
They reduce dependence on sampling long graph random walks via a novel
walk-stitching technique, concatenating several shorter walks without breaking
unbiasedness. By applying these techniques, GRFs++ inherit the approximation
quality provided by longer walks but with greater efficiency, trading
sequential, inefficient sampling of a long walk for parallel computation of
short walks and matrix-matrix multiplication. Furthermore, GRFs++ extend the
simplistic GRFs walk termination mechanism (Bernoulli schemes with fixed
halting probabilities) to a broader class of strategies, applying general
distributions on the walks' lengths. This improves the approximation accuracy
of graph kernels, without incurring extra computational cost. We provide
empirical evaluations to showcase all our claims and complement our results
with theoretical analysis.

</details>


### [35] [DEAS: DEtached value learning with Action Sequence for Scalable Offline RL](https://arxiv.org/abs/2510.07730)
*Changyeon Kim,Haeone Lee,Younggyo Seo,Kimin Lee,Yuke Zhu*

Main category: cs.LG

TL;DR: DEAS是一个简单有效的离线强化学习框架，通过利用动作序列进行价值学习，在复杂长时程任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习虽然避免了昂贵的在线交互，但在复杂长时程顺序决策任务中表现不佳，需要更好的方法来处理长时程规划问题。

Method: 提出DEAS框架，使用动作序列进行价值学习，通过半马尔可夫决策过程Q学习减少有效规划范围，并采用分离价值学习解决价值高估问题。

Result: 在OGBench的复杂长时程任务中持续优于基线方法，并能显著提升大规模视觉-语言-动作模型的性能，在RoboCasa Kitchen仿真和真实世界操作任务中都表现出色。

Conclusion: DEAS通过动作序列和分离价值学习的结合，为离线强化学习在复杂长时程任务中的应用提供了有效的解决方案。

Abstract: Offline reinforcement learning (RL) presents an attractive paradigm for
training intelligent agents without expensive online interactions. However,
current approaches still struggle with complex, long-horizon sequential
decision making. In this work, we introduce DEtached value learning with Action
Sequence (DEAS), a simple yet effective offline RL framework that leverages
action sequences for value learning. These temporally extended actions provide
richer information than single-step actions and can be interpreted through the
options framework via semi-Markov decision process Q-learning, enabling
reduction of the effective planning horizon by considering longer sequences at
once. However, directly adopting such sequences in actor-critic algorithms
introduces excessive value overestimation, which we address through detached
value learning that steers value estimates toward in-distribution actions that
achieve high return in the offline dataset. We demonstrate that DEAS
consistently outperforms baselines on complex, long-horizon tasks from OGBench
and can be applied to enhance the performance of large-scale
Vision-Language-Action models that predict action sequences, significantly
boosting performance in both RoboCasa Kitchen simulation tasks and real-world
manipulation tasks.

</details>


### [36] [MeSH: Memory-as-State-Highways for Recursive Transformers](https://arxiv.org/abs/2510.07739)
*Chengting Yu,Xiaobo Shu,Yadao Wang,Yizhen Zhang,Haoyi Wu,Jiaang Li,Rujiao Long,Ziheng Chen,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 本文提出了MeSH方案，通过外部化状态管理到显式内存缓冲区并使用轻量级路由器动态多样化迭代计算，解决了递归变换器中计算模式单一和信息过载的问题，在Pythia套件上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 递归变换器通过参数重用和隐藏状态迭代解耦计算深度与参数深度，但在相同计算量下，参数较少的递归模型性能往往落后于非递归模型。研究发现这主要源于两个瓶颈：计算模式单一和信息过载。

Method: 引入Memory-as-State-Highways (MeSH)方案，将状态管理外部化到显式内存缓冲区，使用轻量级路由器动态多样化各迭代的计算模式。

Result: 在Pythia套件（160M-1.4B）上，MeSH增强的递归变换器持续优于递归基线，在1.4B规模上超越更大的非递归对应模型，下游准确率平均提升+1.06%，非嵌入参数减少33%。

Conclusion: MeSH是构建更强递归模型的可扩展且有原则的架构，通过诱导迭代间的功能专业化成功解决了递归变换器的病理问题。

Abstract: Recursive transformers reuse parameters and iterate over hidden states
multiple times, decoupling compute depth from parameter depth. However, under
matched compute, recursive models with fewer parameters often lag behind
non-recursive counterparts. By probing hidden states, we trace this performance
gap to two primary bottlenecks: undifferentiated computation, where the core is
forced to adopt a similar computational pattern at every iteration, and
information overload, where long-lived and transient information must coexist
in a single hidden state. To address the issues, we introduce a
Memory-as-State-Highways (MeSH) scheme, which externalizes state management
into an explicit memory buffer and employs lightweight routers to dynamically
diversify computation across iterations. Probing visualizations confirm that
MeSH successfully resolves the pathologies by inducing functional
specialization across iterations. On the Pythia suite (160M-1.4B),
MeSH-enhanced recursive transformers consistently improve over recursive
baselines and outperforms its larger non-recursive counterpart at the 1.4B
scale, improving average downstream accuracy by +1.06% with 33% fewer
non-embedding parameters. Our analysis establishes MeSH as a scalable and
principled architecture for building stronger recursive models.

</details>


### [37] [t-SNE Exaggerates Clusters, Provably](https://arxiv.org/abs/2510.07746)
*Noah Bergam,Szymon Snoeck,Nakul Verma*

Main category: cs.LG

TL;DR: 本文证明t-SNE可视化结果不能可靠反映输入数据的聚类强度和异常点极端程度，揭示了该方法的两个主要失效模式。


<details>
  <summary>Details</summary>
Motivation: t-SNE被广泛使用是基于它能产生与输入结构大致匹配的可视化的信念，但作者发现这种信念存在问题，需要验证t-SNE输出的可靠性。

Method: 通过理论证明和实践演示相结合的方式，分析t-SNE在两个关键方面的失效：输入聚类强度的推断和异常点极端程度的判断。

Result: 研究证明t-SNE输出不能可靠地反映输入数据的聚类强度和异常点的极端程度，这些失效模式在实践中普遍存在。

Conclusion: t-SNE可视化结果在推断输入数据聚类结构和异常点特征方面不可靠，用户需要谨慎解读t-SNE输出结果。

Abstract: Central to the widespread use of t-distributed stochastic neighbor embedding
(t-SNE) is the conviction that it produces visualizations whose structure
roughly matches that of the input. To the contrary, we prove that (1) the
strength of the input clustering, and (2) the extremity of outlier points,
cannot be reliably inferred from the t-SNE output. We demonstrate the
prevalence of these failure modes in practice as well.

</details>


### [38] [FedBook: A Unified Federated Graph Foundation Codebook with Intra-domain and Inter-domain Knowledge Modeling](https://arxiv.org/abs/2510.07755)
*Zhengyu Wu,Yinlin Zhu,Xunkai Li,Ziang Qiu,Rong-Hua Li,Guoren Wang,Chenghu Zhou*

Main category: cs.LG

TL;DR: FedBook是一个联邦图基础模型代码本，通过两阶段过程在联邦预训练中聚合客户端本地代码本，解决现有图基础模型需要集中访问多领域图的隐私限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型通常假设可以集中访问多领域图数据，但这在隐私和机构约束下往往不可行。联邦图基础模型解决了这一限制，但其有效性依赖于构建一个既能实现域内语义一致性又能保持域间多样性的全局代码本。

Method: FedBook采用两阶段过程：1) 域内协作：通过跨客户端参考语义更可靠的高频token来精炼低频token，增强域内一致性；2) 域间集成：在聚合全局图基础模型时，根据客户端代码本的语义独特性加权其贡献，保持跨域多样性。

Result: 在8个基准测试上的广泛实验表明，FedBook在多个领域和任务中持续优于21个基线方法，包括孤立监督学习、联邦学习/联邦图学习、集中式图基础模型的联邦适应以及联邦图基础模型技术。

Conclusion: FedBook通过系统化的联邦代码本聚合方法，有效解决了联邦图基础模型中的域内一致性和域间多样性平衡问题，为隐私保护下的多领域图学习提供了有效解决方案。

Abstract: Foundation models have shown remarkable cross-domain generalization in
language and vision, inspiring the development of graph foundation models
(GFMs). However, existing GFMs typically assume centralized access to
multi-domain graphs, which is often infeasible due to privacy and institutional
constraints. Federated Graph Foundation Models (FedGFMs) address this
limitation, but their effectiveness fundamentally hinges on constructing a
robust global codebook that achieves intra-domain coherence by consolidating
mutually reinforcing semantics within each domain, while also maintaining
inter-domain diversity by retaining heterogeneous knowledge across domains. To
this end, we propose FedBook, a unified federated graph foundation codebook
that systematically aggregates clients' local codebooks during server-side
federated pre-training. FedBook follows a two-phase process: (1) Intra-domain
Collaboration, where low-frequency tokens are refined by referencing more
semantically reliable high-frequency tokens across clients to enhance
domain-specific coherence; and (2) Inter-domain Integration, where client
contributions are weighted by the semantic distinctiveness of their codebooks
during the aggregation of the global GFM, thereby preserving cross-domain
diversity. Extensive experiments on 8 benchmarks across multiple domains and
tasks demonstrate that FedBook consistently outperforms 21 baselines, including
isolated supervised learning, FL/FGL, federated adaptations of centralized
GFMs, and FedGFM techniques.

</details>


### [39] [A Unified Multi-Task Learning Framework for Generative Auto-Bidding with Validation-Aligned Optimization](https://arxiv.org/abs/2510.07760)
*Yiqin Lv,Zhiyu Mou,Miao Xu,Jinghao Chen,Qi Wang,Yixiu Mao,Yun Qu,Rongquan Bai,Chuan Yu,Jian Xu,Bo Zheng,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出VAMO方法，通过基于验证集梯度对齐的自适应任务权重分配，解决在线广告中多任务学习泛化性能差的问题，结合周期性感知模块和生成式自动出价框架提升跨任务季节性结构迁移和出价性能。


<details>
  <summary>Details</summary>
Motivation: 在线广告中异构广告主需求产生大量定制化出价任务，传统独立优化方法计算量大且数据效率低。现有多任务学习方法主要基于训练动态，在波动性出价环境中泛化性能差。

Method: 提出验证对齐多任务优化(VAMO)，根据每个任务训练梯度与验证梯度的对齐度自适应分配任务权重，引导更新方向朝向验证集改进。结合周期性感知时序模块和生成式自动出价框架，增强跨任务季节性结构迁移。

Result: 在模拟和真实大规模广告系统上的广泛实验表明，相比典型基线方法有显著改进，验证了所提方法的有效性。

Conclusion: VAMO方法通过验证梯度对齐机制有效提升了多任务学习在波动性广告环境中的泛化性能，为在线广告优化提供了有效的解决方案。

Abstract: In online advertising, heterogeneous advertiser requirements give rise to
numerous customized bidding tasks that are typically optimized independently,
resulting in extensive computation and limited data efficiency. Multi-task
learning offers a principled framework to train these tasks jointly through
shared representations. However, existing multi-task optimization strategies
are primarily guided by training dynamics and often generalize poorly in
volatile bidding environments. To this end, we present Validation-Aligned
Multi-task Optimization (VAMO), which adaptively assigns task weights based on
the alignment between per-task training gradients and a held-out validation
gradient, thereby steering updates toward validation improvement and better
matching deployment objectives. We further equip the framework with a
periodicity-aware temporal module and couple it with an advanced generative
auto-bidding backbone to enhance cross-task transfer of seasonal structure and
strengthen bidding performance. Meanwhile, we provide theoretical insights into
the proposed method, e.g., convergence guarantee and alignment analysis.
Extensive experiments on both simulated and large-scale real-world advertising
systems consistently demonstrate significant improvements over typical
baselines, illuminating the effectiveness of the proposed approach.

</details>


### [40] [FedLAM: Low-latency Wireless Federated Learning via Layer-wise Adaptive Modulation](https://arxiv.org/abs/2510.07766)
*Linping Qu,Shenghui Song,Chi-Ying Tsui*

Main category: cs.LG

TL;DR: 提出一种层自适应调制方案，通过为不同DNN层分配不同调制级别来减少无线联邦学习中的通信延迟。


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习中，客户端需要通过带宽受限的通道传输高维DNN参数，导致通信延迟问题。现有方法为所有DNN层分配相同调制级别，限制了延迟优化的自由度。

Method: 考虑DNN层的重要性，提出层自适应调制方案，自动决定不同DNN层的最优调制级别。

Result: 实验结果显示，与现有方案相比，所提方案可节省高达73.9%的通信延迟。

Conclusion: 层自适应调制方案能有效减少无线联邦学习中的通信延迟，通过考虑层重要性提供更大的优化空间。

Abstract: In wireless federated learning (FL), the clients need to transmit the
high-dimensional deep neural network (DNN) parameters through bandwidth-limited
channels, which causes the communication latency issue. In this paper, we
propose a layer-wise adaptive modulation scheme to save the communication
latency. Unlike existing works which assign the same modulation level for all
DNN layers, we consider the layers' importance which provides more freedom to
save the latency. The proposed scheme can automatically decide the optimal
modulation levels for different DNN layers. Experimental results show that the
proposed scheme can save up to 73.9% of communication latency compared with the
existing schemes.

</details>


### [41] [Weak Form Learning for Mean-Field Partial Differential Equations: an Application to Insect Movement](https://arxiv.org/abs/2510.07786)
*Seth Minor,Bret D. Elderd,Benjamin Van Allen,David M. Bortz,Vanja Dukic*

Main category: cs.LG

TL;DR: 该研究扩展弱形式方程学习技术，结合核密度估计，从稀疏实验数据中学习鳞翅目幼虫种群运动的有效模型，应用于模拟农业条件下的秋粘虫位置测量数据。


<details>
  <summary>Details</summary>
Motivation: 理解作物和林业害虫的扩散动态可以更好地预测爆发强度和位置，从而实现更好的害虫管理。昆虫在感染、捕食和各向异性环境条件下可能表现出优先运动模式。

Method: 扩展弱形式方程学习技术（WSINDy算法），结合核密度估计，从稀疏的昆虫位置测量数据中学习福克-普朗克方程。

Result: 在具有不同植物资源和感染状态的模拟农业条件下，成功应用于秋粘虫位置测量的稀疏数据集，证明了该方法的有效性。

Conclusion: 弱形式方程学习技术结合核密度估计能够从稀疏实验数据中有效学习昆虫种群运动模型，为害虫管理提供预测工具。

Abstract: Insect species subject to infection, predation, and anisotropic environmental
conditions may exhibit preferential movement patterns. Given the innate
stochasticity of exogenous factors driving these patterns over short
timescales, individual insect trajectories typically obey overdamped stochastic
dynamics. In practice, data-driven modeling approaches designed to learn the
underlying Fokker-Planck equations from observed insect distributions serve as
ideal tools for understanding and predicting such behavior. Understanding
dispersal dynamics of crop and silvicultural pests can lead to a better
forecasting of outbreak intensity and location, which can result in better pest
management. In this work, we extend weak-form equation learning techniques,
coupled with kernel density estimation, to learn effective models for
lepidopteran larval population movement from highly sparse experimental data.
Galerkin methods such as the Weak form Sparse Identification of Nonlinear
Dynamics (WSINDy) algorithm have recently proven useful for learning governing
equations in several scientific contexts. We demonstrate the utility of the
method on a sparse dataset of position measurements of fall armyworms
(Spodoptera frugiperda) obtained in simulated agricultural conditions with
varied plant resources and infection status.

</details>


### [42] [SIMU: Selective Influence Machine Unlearning](https://arxiv.org/abs/2510.07822)
*Anu Agarwal,Mihir Pamnani,Dilek Hakkani-Tur*

Main category: cs.LG

TL;DR: 提出了SIMU框架，通过选择性更新关键神经元来增强二阶优化器基础的机器遗忘方法，在保持遗忘效果的同时显著减少对模型原有能力的损害。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在让大语言模型遗忘敏感信息时，往往会损害模型的原有能力和知识，需要一种既能有效遗忘又能保持模型效用的方法。

Method: 采用两阶段框架：首先识别对遗忘集编码起关键作用的神经元，然后仅更新这些目标神经元，约束更新范围。

Result: SIMU在实现可比较的遗忘效果的同时，在保持模型原有知识方面显著优于现有方法。

Conclusion: 选择性影响机器遗忘框架通过限制神经元更新范围，成功平衡了遗忘效果与模型能力保持，为安全可靠的机器遗忘提供了有效解决方案。

Abstract: The undesired memorization of sensitive information by Large Language Models
(LLMs) has emphasized the need for safety mechanisms that can regulate model
behavior. This has led to the development of machine unlearning techniques that
enable models to precisely forget sensitive and unwanted information. For
machine unlearning, first-order and second-order optimizer-based methods have
shown significant progress in enabling LLMs to forget targeted information.
However, in doing so, these approaches often compromise the model's original
capabilities, resulting in unlearned models that struggle to retain their prior
knowledge and overall utility. To address this, we propose Selective Influence
Machine Unlearning (SIMU), a two-step framework that enhances second-order
optimizer-based unlearning by selectively updating only the critical neurons
responsible for encoding the forget-set. By constraining updates to these
targeted neurons, SIMU achieves comparable unlearning efficacy while
substantially outperforming current methods in retaining the model's original
knowledge.

</details>


### [43] [MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation](https://arxiv.org/abs/2510.07835)
*Weisen Jiang,Sinno Jialin Pan*

Main category: cs.LG

TL;DR: MetaDefense是一个针对大语言模型微调越狱攻击的防御框架，通过预生成检测和生成中监控两阶段方法，在多个LLM架构上显著优于现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制无法泛化到使用未见攻击模板伪装的恶意查询，尽管LLM在嵌入空间中能够区分这些伪装查询。

Method: 提出两阶段防御方法：预生成阶段检测恶意查询，生成中阶段监控部分响应以防止输出更多有害内容；通过专门提示训练LLM预测查询和部分响应的有害性。

Result: 在多个LLM架构上的广泛实验表明，MetaDefense显著优于现有防御机制，对使用已见和未见攻击模板的恶意查询实现鲁棒防御，同时在良性任务上保持竞争力。

Conclusion: MetaDefense通过两阶段防御方法有效抵御微调越狱攻击，为LLM安全提供了新的解决方案。

Abstract: This paper introduces MetaDefense, a novel framework for defending against
finetuning-based jailbreak attacks in large language models (LLMs). We observe
that existing defense mechanisms fail to generalize to harmful queries
disguised by unseen attack templates, despite LLMs being capable of
distinguishing disguised harmful queries in the embedding space. Based on these
insights, we propose a two-stage defense approach: (i) pre-generation defense
that detects harmful queries before response generation begins, and (ii)
mid-generation defense that monitors partial responses during generation to
prevent outputting more harmful content. Our MetaDefense trains the LLM to
predict the harmfulness of both queries and partial responses using specialized
prompts, enabling early termination of potentially harmful interactions.
Extensive experiments across multiple LLM architectures (LLaMA-2-7B,
Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense
significantly outperforms existing defense mechanisms, achieving robust defense
against harmful queries with seen and unseen attack templates while maintaining
competitive performance on benign tasks. Code is available at
https://github.com/ws-jiang/MetaDefense.

</details>


### [44] [Self-Improving LLM Agents at Test-Time](https://arxiv.org/abs/2510.07841)
*Emre Can Acikgoz,Cheng Qian,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.LG

TL;DR: 提出了一种测试时自我改进方法，通过识别模型不确定的样本、生成类似示例并进行测试时微调，显著提升模型性能且大幅减少训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型微调依赖大量训练数据，但数据收集效率低、训练成本高，且无法保证模型能处理复杂场景或更好泛化。现有技术很少评估训练样本是否提供新信息或与模型已有知识冗余。

Method: 提出三步算法：1）识别模型难以处理的样本（自我意识）；2）从不确定样本生成类似示例（自我数据增强）；3）在测试时使用新生成样本进行微调（自我改进）。研究两种变体：TT-SI（同一模型生成并学习）和TT-D（更强模型生成供学生模型学习）。

Result: 在多个智能体基准测试中，TT-SI平均绝对准确率提升5.48%，超越其他标准学习方法，且仅使用1/68的训练样本。

Conclusion: 测试时自我改进算法展示了构建更强大智能体向自我进化发展的新范式潜力。

Abstract: One paradigm of language model (LM) fine-tuning relies on creating large
training datasets, under the assumption that high quantity and diversity will
enable models to generalize to novel tasks after post-training. In practice,
gathering large sets of data is inefficient, and training on them is
prohibitively expensive; worse, there is no guarantee that the resulting model
will handle complex scenarios or generalize better. Moreover, existing
techniques rarely assess whether a training sample provides novel information
or is redundant with the knowledge already acquired by the model, resulting in
unnecessary costs. In this work, we explore a new test-time self-improvement
method to create more effective and generalizable agentic LMs on-the-fly. The
proposed algorithm can be summarized in three steps: (i) first it identifies
the samples that model struggles with (self-awareness), (ii) then generates
similar examples from detected uncertain samples (self-data augmentation), and
(iii) uses these newly generated samples at test-time fine-tuning
(self-improvement). We study two variants of this approach: Test-Time
Self-Improvement (TT-SI), where the same model generates additional training
examples from its own uncertain cases and then learns from them, and contrast
this approach with Test-Time Distillation (TT-D), where a stronger model
generates similar examples for uncertain cases, enabling student to adapt using
distilled supervision. Empirical evaluations across different agent benchmarks
demonstrate that TT-SI improves the performance with +5.48% absolute accuracy
gain on average across all benchmarks and surpasses other standard learning
methods, yet using 68x less training samples. Our findings highlight the
promise of TT-SI, demonstrating the potential of self-improvement algorithms at
test-time as a new paradigm for building more capable agents toward
self-evolution.

</details>


### [45] [Signal-to-Noise Ratio in Scanning Electron Microscopy: A Comprehensive Review](https://arxiv.org/abs/2510.07886)
*K. S. Sim,I. Bukhori,D. C. Y. Ong,K. B. Gan*

Main category: cs.LG

TL;DR: 这篇综述论文探讨了扫描电子显微镜(SEM)中的信噪比(SNR)问题，涵盖了SEM工作原理、噪声来源、SNR测量方法以及硬件和软件层面的SNR优化技术。


<details>
  <summary>Details</summary>
Motivation: SEM在纳米技术、材料科学和生物成像中至关重要，但噪声会降低图像质量。信噪比是影响SEM图像质量和可解释性的关键参数，需要系统研究如何优化SNR。

Method: 通过综述分析的方法，从SEM基本原理、噪声来源、SNR测量估计方法，到影响SNR测量的各种因素以及硬件和软件层面的SNR增强方法。

Result: 回顾了传统和新兴的SNR优化技术，分析了它们的应用、优势和局限性，为研究人员提供了全面的SNR优化理解。

Conclusion: 论文旨在为研究人员和从业者提供SEM中SNR优化的全面理解，并鼓励该领域的进一步研究。

Abstract: Scanning Electron Microscopy (SEM) is critical in nanotechnology, materials
science, and biological imaging due to its high spatial resolution and depth of
focus. Signal-to-noise ratio (SNR) is an essential parameter in SEM because it
directly impacts the quality and interpretability of the images. SEM is widely
used in various scientific disciplines, but its utility can be compromised by
noise, which degrades image clarity. This review explores multiple aspects of
the SEM imaging process, from the principal operation of SEM, sources of noise
in SEM, methods for SNR measurement and estimations, to various aspects that
affect the SNR measurement and approaches to enhance SNR, both from a hardware
and software standpoint. We review traditional and emerging techniques,
focusing on their applications, advantages, and limitations. The paper aims to
provide a comprehensive understanding of SNR optimization in SEM for
researchers and practitioners and to encourage further research in the field.

</details>


### [46] [Adaptive Optimizable Gaussian Process Regression Linear Least Squares Regression Filtering Method for SEM Images](https://arxiv.org/abs/2510.07895)
*D. Chee Yong Ong,I. Bukhori,K. S. Sim,K. Beng Gan*

Main category: cs.LG

TL;DR: 该研究提出了一种完整的SEM图像信噪比和噪声方差估计方法，并使用噪声方差引导的维纳滤波来增强图像质量。通过比较多种SNR估计技术，发现LSR方法表现最佳，然后结合优化的GPR模型进行NV估计，最终提出了AO-GPRLLSR滤波管道。


<details>
  <summary>Details</summary>
Motivation: SEM图像常受到噪声污染，这会降低图像质量并影响后续分析。需要开发更准确的信噪比和噪声方差估计方法，以指导有效的噪声去除。

Method: 首先比较了五种SNR估计技术（NN、FOL、NN+FOL、NLLSR、LSR），发现LSR方法最优。然后测试了SVM和GPR模型与LSR配对，其中优化的GPR模型在NV估计中表现最佳。最终提出了AO-GPRLLSR滤波管道，将估计的噪声方差输入到NV引导的维纳滤波器中。

Result: LSR方法在SNR估计中表现最佳，优化的GPR模型在NV估计中准确度最高。提出的AO-GPRLLSR方法在滤波过程中实现了更低的均方误差，显著提高了SEM图像质量。

Conclusion: 该研究成功开发了一种有效的SEM图像噪声估计和去除方法，结合优化的机器学习模型和维纳滤波，能够显著提升SEM图像质量并降低滤波误差。

Abstract: Scanning Electron Microscopy (SEM) images often suffer from noise
contamination, which degrades image quality and affects further analysis. This
research presents a complete approach to estimate their Signal-to-Noise Ratio
(SNR) and noise variance (NV), and enhance image quality using NV-guided Wiener
filter. The main idea of this study is to use a good SNR estimation technique
and infuse a machine learning model to estimate NV of the SEM image, which then
guides the wiener filter to remove the noise, providing a more robust and
accurate SEM image filtering pipeline. First, we investigate five different SNR
estimation techniques, namely Nearest Neighbourhood (NN) method, First-Order
Linear Interpolation (FOL) method, Nearest Neighbourhood with First-Order
Linear Interpolation (NN+FOL) method, Non-Linear Least Squares Regression
(NLLSR) method, and Linear Least Squares Regression (LSR) method. It is shown
that LSR method to perform better than the rest. Then, Support Vector Machines
(SVM) and Gaussian Process Regression (GPR) are tested by pairing it with LSR.
In this test, the Optimizable GPR model shows the highest accuracy and it
stands as the most effective solution for NV estimation. Combining these
results lead to the proposed Adaptive Optimizable Gaussian Process Regression
Linear Least Squares Regression (AO-GPRLLSR) Filtering pipeline. The AO-GPRLLSR
method generated an estimated noise variance which served as input to NV-guided
Wiener filter for improving the quality of SEM images. The proposed method is
shown to achieve notable success in estimating SNR and NV of SEM images and
leads to lower Mean Squared Error (MSE) after the filtering process.

</details>


### [47] [GRADE: Personalized Multi-Task Fusion via Group-relative Reinforcement Learning with Adaptive Dirichlet Exploratio](https://arxiv.org/abs/2510.07919)
*Tingfeng Hong,Pingye Ren,Xinlong Xiao,Chao Wang,Chenyi Lei,Wenwu Ou,Han Li*

Main category: cs.LG

TL;DR: 提出了一种个性化多目标排序系统的整体架构，包括特征中心与预排序模型、多任务学习模型、多任务融合模块（GRADE框架）以及混合排序模型，用于生成个性化权重并计算最终排序分数。


<details>
  <summary>Details</summary>
Motivation: 为了解决多目标排序中不同用户对各项指标的偏好差异，需要开发能够学习个性化权重的系统，以更好地满足用户多样化的需求。

Method: 系统包含四个主要组件：特征中心与预排序模型进行初始特征处理和候选生成；多任务学习模型预测多种用户反馈信号；多任务融合模块（GRADE框架）学习个性化权重；混合排序模型应用权重计算最终分数并生成排序结果。

Result: 通过GRADE框架学习到的个性化权重能够有效应用于最终评分计算，生成符合用户偏好的混合排序结果。

Conclusion: 该个性化多目标排序系统架构能够有效处理用户多样化的偏好需求，通过个性化权重学习实现更精准的排序结果交付。

Abstract: Overall architecture of the personalized multi-objective ranking system. It
comprises: (1) a Feature Center and Prerank Model for initial feature
processing and candidate generation; (2) a Multi-Task Learning (MTL) model
predicting various user feedback signals; (3) a Multi-Task Fusion (MTF) module
(our proposed GRADE framework) that learns personalized weights ($w_1, \dots,
w_n$); these weights are then applied to calculate final scores and sorted to
generate a blended ranking by the Blended Ranking Model, which ultimately
delivers results to users.

</details>


### [48] [DISCO: Diversifying Sample Condensation for Efficient Model Evaluation](https://arxiv.org/abs/2510.07959)
*Alexander Rubinstein,Benjamin Raible,Martin Gubri,Seong Joon Oh*

Main category: cs.LG

TL;DR: DISCO提出了一种新的机器学习模型评估方法，通过选择模型响应分歧最大的样本来预测最终测试性能，避免了传统基于聚类的复杂锚点子集选择过程。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型评估成本过高，降低了包容性、减缓了创新周期并加剧了环境影响。传统方法依赖聚类选择锚点子集，过程复杂且对设计选择敏感。

Method: DISCO方法选择模型响应分歧最大的前k个样本，使用贪心的样本级统计而非全局聚类，概念更简单。从理论角度看，模型间分歧为这种贪心选择提供了信息理论上的最优规则。

Result: DISCO在MMLU、Hellaswag、Winogrande和ARC等基准测试中实现了性能预测的最先进结果，优于先前方法。

Conclusion: 促进样本多样性并不重要，重要的是选择那些能最大化模型响应多样性的样本。DISCO方法在概念上更简单，在理论上更优，并在实践中表现出色。

Abstract: Evaluating modern machine learning models has become prohibitively expensive.
Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model.
Costly evaluation reduces inclusivity, slows the cycle of innovation, and
worsens environmental impact. The typical approach follows two steps. First,
select an anchor subset of data. Second, train a mapping from the accuracy on
this subset to the final test result. The drawback is that anchor selection
depends on clustering, which can be complex and sensitive to design choices. We
argue that promoting diversity among samples is not essential; what matters is
to select samples that $\textit{maximise diversity in model responses}$. Our
method, $\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k
samples with the greatest model disagreements. This uses greedy, sample-wise
statistics rather than global clustering. The approach is conceptually simpler.
From a theoretical view, inter-model disagreement provides an
information-theoretically optimal rule for such greedy selection.
$\textbf{DISCO}$ shows empirical gains over prior methods, achieving
state-of-the-art results in performance prediction across MMLU, Hellaswag,
Winogrande, and ARC. Code is available here:
https://github.com/arubique/disco-public.

</details>


### [49] [PRESCRIBE: Predicting Single-Cell Responses with Bayesian Estimation](https://arxiv.org/abs/2510.07964)
*Jiabei Cheng,Changxi Chi,Jingbo Zhou,Hongyi Xin,Jun Xia*

Main category: cs.LG

TL;DR: PRESCRIBE是一个多变量深度证据回归框架，用于单细胞扰动预测中联合测量模型不确定性和数据不确定性，通过置信度评分提高预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 单细胞扰动预测中，预测训练数据中未见基因的效果时，需要考虑模型不确定性（目标基因与训练基因的相似性）和数据不确定性（训练数据质量），这两者对于确定预测可靠性至关重要。

Method: 提出PRESCRIBE（PREdicting Single-Cell Response wIth Bayesian Estimation）多变量深度证据回归框架，能够联合测量两种不确定性来源。

Result: PRESCRIBE能够有效估计每个预测的置信度评分，该评分与经验准确性高度相关，能够过滤不可靠结果，在实验中相比可比基线实现了超过3%的稳定准确性提升。

Conclusion: PRESCRIBE通过联合估计模型和数据不确定性，为单细胞扰动预测提供了可靠的置信度评估，显著提高了预测的准确性和可靠性。

Abstract: In single-cell perturbation prediction, a central task is to forecast the
effects of perturbing a gene unseen in the training data. The efficacy of such
predictions depends on two factors: (1) the similarity of the target gene to
those covered in the training data, which informs model (epistemic)
uncertainty, and (2) the quality of the corresponding training data, which
reflects data (aleatoric) uncertainty. Both factors are critical for
determining the reliability of a prediction, particularly as gene perturbation
is an inherently stochastic biochemical process. In this paper, we propose
PRESCRIBE (PREdicting Single-Cell Response wIth Bayesian Estimation), a
multivariate deep evidential regression framework designed to measure both
sources of uncertainty jointly. Our analysis demonstrates that PRESCRIBE
effectively estimates a confidence score for each prediction, which strongly
correlates with its empirical accuracy. This capability enables the filtering
of untrustworthy results, and in our experiments, it achieves steady accuracy
improvements of over 3% compared to comparable baselines.

</details>


### [50] [Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training](https://arxiv.org/abs/2510.07980)
*Qinglun Li,Yingqi Liu,Miao Zhang,Xiaochun Cao,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: 本文通过稳定性分析推导了多Gossip步骤(MGS)的泛化误差和超额误差上界，系统回答了MGS有效性的理论原因以及能否完全消除去中心化与中心化训练之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 去中心化训练虽然通信效率高，但性能通常低于中心化训练。MGS作为简单有效的桥梁，能显著减少性能差距，但其有效性理论原因及能否完全消除差距仍是未解问题。

Method: 使用稳定性分析推导MGS的泛化误差和超额误差上界，分析学习率、数据异构性、节点数量、每节点样本大小和通信拓扑等因素对MGS泛化的影响。

Result: 1) MGS以指数速率减少优化误差界，从而指数收紧泛化误差界；2) 即使MGS趋于无穷，去中心化与中心化训练之间仍存在不可忽略的泛化误差差距；3) 在CIFAR数据集上的实验验证了理论发现。

Conclusion: MGS能显著改善去中心化训练性能，但无法完全消除与中心化训练的性能差距，为去中心化训练提供了重要的理论指导。

Abstract: Decentralized training removes the centralized server, making it a
communication-efficient approach that can significantly improve training
efficiency, but it often suffers from degraded performance compared to
centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective
bridge between decentralized and centralized training, significantly reducing
experiment performance gaps. However, the theoretical reasons for its
effectiveness and whether this gap can be fully eliminated by MGS remain open
questions. In this paper, we derive upper bounds on the generalization error
and excess error of MGS using stability analysis, systematically answering
these two key questions. 1). Optimization Error Reduction: MGS reduces the
optimization error bound at an exponential rate, thereby exponentially
tightening the generalization error bound and enabling convergence to better
solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a
non-negligible gap in generalization error remains compared to centralized
mini-batch SGD ($\mathcal{O}(T^{\frac{c\beta}{c\beta +1}}/{n m})$ in
centralized and $\mathcal{O}(T^{\frac{2c\beta}{2c\beta +2}}/{n
m^{\frac{1}{2c\beta +2}}})$ in decentralized). Furthermore, we provide the
first unified analysis of how factors like learning rate, data heterogeneity,
node count, per-node sample size, and communication topology impact the
generalization of MGS under non-convex settings without the bounded gradients
assumption, filling a critical theoretical gap in decentralized training.
Finally, promising experiments on CIFAR datasets support our theoretical
findings.

</details>


### [51] [DemandCast: Global hourly electricity demand forecasting](https://arxiv.org/abs/2510.08000)
*Kevin Steijn,Vamsi Priya Goli,Enrico Antonini*

Main category: cs.LG

TL;DR: 本文提出了一个基于XGBoost梯度提升算法的机器学习框架，用于跨地理区域的电力需求预测，整合历史需求、天气和社会经济变量来预测标准化电力需求曲线。


<details>
  <summary>Details</summary>
Motivation: 为能源系统规划者和政策制定者提供准确且可扩展的电力需求预测，以应对全球能源转型的挑战。

Method: 使用XGBoost算法，整合历史电力需求、天气和社会经济变量，采用时间数据分割策略确保样本外性能评估，构建跨多年多国家的大规模数据集。

Result: 该方法能够提供准确且可扩展的需求预测。

Conclusion: 该框架为能源系统规划提供了有价值的见解，有助于应对全球能源转型的挑战。

Abstract: This paper presents a machine learning framework for electricity demand
forecasting across diverse geographical regions using the gradient boosting
algorithm XGBoost. The model integrates historical electricity demand and
comprehensive weather and socioeconomic variables to predict normalized
electricity demand profiles. To enable robust training and evaluation, we
developed a large-scale dataset spanning multiple years and countries, applying
a temporal data-splitting strategy that ensures benchmarking of out-of-sample
performance. Our approach delivers accurate and scalable demand forecasts,
providing valuable insights for energy system planners and policymakers as they
navigate the challenges of the global energy transition.

</details>


### [52] [Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training](https://arxiv.org/abs/2510.08008)
*Ruizhe Wang,Yucheng Ding,Xiao Liu,Yaoxiang Wang,Peng Cheng,Baining Guo,Zhengjun Zha,Yeyun Gong*

Main category: cs.LG

TL;DR: 提出了一种通过参数扩展和继续训练来回收预训练检查点的方法，包括深度增长的层间复制和宽度增长的专家复制加噪声注入，在相同额外计算预算下比从头训练获得10.66%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练的计算成本快速增长，而许多已训练好的检查点由于工程限制或模型容量有限而未被充分利用，需要高效重用这些"沉没"成本。

Method: 提出正交增长方法：深度增长采用层间复制，宽度增长采用专家复制并注入噪声；通过全面扩展实验确定检查点序列的最佳增长时机。

Result: 扩展到70B参数和超过1T训练标记的模型，在相同额外计算预算下比从头训练获得10.66%的准确率提升；最终准确率与沉没成本量呈强正相关。

Conclusion: 检查点回收方法为经济高效的大型语言模型预训练奠定了基础。

Abstract: The rapidly increasing computational cost of pretraining Large Language
Models necessitates more efficient approaches. Numerous computational costs
have been invested in existing well-trained checkpoints, but many of them
remain underutilized due to engineering constraints or limited model capacity.
To efficiently reuse this "sunk" cost, we propose to recycle pretrained
checkpoints by expanding their parameter counts and continuing training. We
propose orthogonal growth method well-suited for converged Mixture-of-Experts
model: interpositional layer copying for depth growth and expert duplication
with injected noise for width growth. To determine the optimal timing for such
growth across checkpoints sequences, we perform comprehensive scaling
experiments revealing that the final accuracy has a strong positive correlation
with the amount of sunk cost, indicating that greater prior investment leads to
better performance. We scale our approach to models with 70B parameters and
over 1T training tokens, achieving 10.66% accuracy gain over training from
scratch under the same additional compute budget. Our checkpoint recycling
approach establishes a foundation for economically efficient large language
model pretraining.

</details>


### [53] [DUA-D2C: Dynamic Uncertainty Aware Method for Overfitting Remediation in Deep Learning](https://arxiv.org/abs/2411.15876)
*Md. Saiful Bari Siddiqui,Md Mohaiminul Islam,Md. Golam Rabiul Alam*

Main category: cs.LG

TL;DR: 本文提出了动态不确定性感知的Divide2Conquer方法（DUA-D2C），通过基于验证集性能动态加权子集模型的贡献来改进聚合过程，有效对抗深度学习中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中的过拟合问题，特别是当面对数据异常值、噪声和有限训练数据时。传统的D2C方法在聚合时对所有子集模型平等对待，未能充分利用它们不同的泛化能力。

Method: 在D2C方法基础上，引入动态不确定性感知机制，基于子集模型在共享验证集上的准确率和预测不确定性来动态加权它们的贡献，使中心模型优先学习那些产生更可泛化和置信度更高的边缘模型。

Result: 在多个领域的基准数据集上的实证评估表明，DUA-D2C显著提高了泛化性能。分析包括决策边界、损失曲线和其他性能指标的评估，突出了DUA-D2C的有效性。

Conclusion: DUA-D2C即使在应用于其他正则化方法之上时也能提高泛化性能，使其成为对抗现代深度学习中过拟合问题的理论上有基础且有效的方法。

Abstract: Overfitting remains a significant challenge in deep learning, often arising
from data outliers, noise, and limited training data. To address this, the
Divide2Conquer (D2C) method was previously proposed, which partitions training
data into multiple subsets and trains identical models independently on each.
This strategy enables learning more consistent patterns while minimizing the
influence of individual outliers and noise. However, D2C's standard aggregation
typically treats all subset models equally or based on fixed heuristics (like
data size), potentially underutilizing information about their varying
generalization capabilities. Building upon this foundation, we introduce
Dynamic Uncertainty-Aware Divide2Conquer (DUA-D2C), an advanced technique that
refines the aggregation process. DUA-D2C dynamically weights the contributions
of subset models based on their performance on a shared validation set,
considering both accuracy and prediction uncertainty. This intelligent
aggregation allows the central model to preferentially learn from subsets
yielding more generalizable and confident edge models, thereby more effectively
combating overfitting. Empirical evaluations on benchmark datasets spanning
multiple domains demonstrate that DUA-D2C significantly improves
generalization. Our analysis includes evaluations of decision boundaries, loss
curves, and other performance metrics, highlighting the effectiveness of
DUA-D2C. This study demonstrates that DUA-D2C improves generalization
performance even when applied on top of other regularization methods,
establishing it as a theoretically grounded and effective approach to combating
overfitting in modern deep learning. Our codes are publicly available at:
https://github.com/Saiful185/DUA-D2C.

</details>


### [54] [Accelerated Evolving Set Processes for Local PageRank Computation](https://arxiv.org/abs/2510.08010)
*Binbin Huang,Luo Luo,Yanghua Xiao,Deqing Yang,Baojian Zhou*

Main category: cs.LG

TL;DR: 提出基于嵌套演化集过程的框架来加速个性化PageRank计算，通过局部化近似近端点迭代求解简化线性系统，时间复杂度为min{Õ(R²/ε²), Õ(m)}，其中R为常数，m为边数。


<details>
  <summary>Details</summary>
Motivation: 解决现有个性化PageRank计算方法的效率问题，特别是在大规模图上的计算复杂度，旨在开发不依赖于图规模的快速近似算法。

Method: 使用嵌套演化集过程框架，在每个阶段采用局部化近似近端点迭代来求解简化的线性系统，将问题分解为Õ(1/√α)个线性系统求解。

Result: 当1/ε² ≪ m时，算法整体时间复杂度为Õ(R²/(√αε²))，与图规模无关。实验验证了方法在真实图上的高效性，早期阶段即表现出显著收敛。

Conclusion: 该框架成功解决了文献中的开放猜想，提供了独立于图规模的个性化PageRank高效近似算法，在理论和实验层面均表现出优越性能。

Abstract: This work proposes a novel framework based on nested evolving set processes
to accelerate Personalized PageRank (PPR) computation. At each stage of the
process, we employ a localized inexact proximal point iteration to solve a
simplified linear system. We show that the time complexity of such localized
methods is upper bounded by $\min\{\tilde{\mathcal{O}}(R^2/\epsilon^2),
\tilde{\mathcal{O}}(m)\}$ to obtain an $\epsilon$-approximation of the PPR
vector, where $m$ denotes the number of edges in the graph and $R$ is a
constant defined via nested evolving set processes. Furthermore, the algorithms
induced by our framework require solving only
$\tilde{\mathcal{O}}(1/\sqrt{\alpha})$ such linear systems, where $\alpha$ is
the damping factor. When $1/\epsilon^2\ll m$, this implies the existence of an
algorithm that computes an $\ epsilon $-approximation of the PPR vector with an
overall time complexity of $\tilde{\mathcal{O}}\left(R^2 /
(\sqrt{\alpha}\epsilon^2)\right)$, independent of the underlying graph size.
Our result resolves an open conjecture from existing literature. Experimental
results on real-world graphs validate the efficiency of our methods,
demonstrating significant convergence in the early stages.

</details>


### [55] [Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses](https://arxiv.org/abs/2510.08016)
*Stanisław Pawlak,Jan Dubiński,Daniel Marczak,Bartłomiej Twardowski*

Main category: cs.LG

TL;DR: 本文提出了一种将后门攻击视为任务向量的框架，通过后门向量(BV)分析攻击特性，并开发了稀疏后门向量(SBV)增强攻击效果，同时提出了注入BV减法(IBVS)作为轻量级防御方法。


<details>
  <summary>Details</summary>
Motivation: 模型合并技术虽然有效，但存在严重的安全风险，特别是容易受到后门攻击。现有研究显示攻击者可以通过在单个微调模型中植入隐藏触发器来控制最终合并模型的输出。

Method: 1) 将后门攻击建模为任务向量，计算后门向量(BV)；2) 提出稀疏后门向量(SBV)方法，将多个攻击合并为单一攻击；3) 开发注入BV减法(IBVS)防御机制，无需假设即可对抗后门威胁。

Result: SBV方法超越了之前的攻击效果，是首个利用合并技术提升后门有效性的方法。同时，IBVS提供了轻量级通用防御，即使在完全未知后门威胁的情况下仍保持有效。

Conclusion: 该研究揭示了模型合并中后门威胁的核心漏洞——利用基础模型中对抗性弱点的固有触发器，并提出了有效的攻击和防御框架，为模型合并安全性提供了新的理解和解决方案。

Abstract: Model merging (MM) recently emerged as an effective method for combining
large deep learning models. However, it poses significant security risks.
Recent research shows that it is highly susceptible to backdoor attacks, which
introduce a hidden trigger into a single fine-tuned model instance that allows
the adversary to control the output of the final merged model at inference
time. In this work, we propose a simple framework for understanding backdoor
attacks by treating the attack itself as a task vector. $Backdoor\ Vector\
(BV)$ is calculated as the difference between the weights of a fine-tuned
backdoored model and fine-tuned clean model. BVs reveal new insights into
attacks understanding and a more effective framework to measure their
similarity and transferability. Furthermore, we propose a novel method that
enhances backdoor resilience through merging dubbed $Sparse\ Backdoor\ Vector\
(SBV)$ that combines multiple attacks into a single one. We identify the core
vulnerability behind backdoor threats in MM: $inherent\ triggers$ that exploit
adversarial weaknesses in the base model. To counter this, we propose
$Injection\ BV\ Subtraction\ (IBVS)$ - an assumption-free defense against
backdoors in MM. Our results show that SBVs surpass prior attacks and is the
first method to leverage merging to improve backdoor effectiveness. At the same
time, IBVS provides a lightweight, general defense that remains effective even
when the backdoor threat is entirely unknown.

</details>


### [56] [Do We Really Need Permutations? Impact of Width Expansion on Linear Mode Connectivity](https://arxiv.org/abs/2510.08023)
*Akira Ito,Masanori Yamada,Daiki Chijiwa,Atsutoshi Kumagai*

Main category: cs.LG

TL;DR: 该论文表明，即使不使用参数排列，仅通过加宽模型并使用合适的softmax温度校准，就能实现线性模式连接。作者通过分析中间层输出，引入了层间指数加权连接性来解释这一现象。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为实现线性模式连接需要参数排列搜索和足够宽的模型。本文旨在验证是否仅通过加宽模型就能实现线性模式连接，而无需复杂的排列搜索。

Method: 使用softmax温度校准技术，分析中间层输出，引入层间指数加权连接性概念来解释模型合并后的输出行为。

Result: 实验证明，即使不使用任何参数排列，仅通过加宽模型并采用合适的softmax温度校准，就能成功实现线性模式连接。

Conclusion: 模型加宽不仅有助于非线性模式连接，还能显著提高实现线性模式连接的可能性，这为模型合并提供了更简单有效的方法。

Abstract: Recently, Ainsworth et al. empirically demonstrated that, given two
independently trained models, applying a parameter permutation that preserves
the input-output behavior allows the two models to be connected by a low-loss
linear path. When such a path exists, the models are said to achieve linear
mode connectivity (LMC). Prior studies, including Ainsworth et al., have
reported that achieving LMC requires not only an appropriate permutation search
but also sufficiently wide models (e.g., a 32 $\times$ width multiplier for
ResNet-20). This is broadly believed to be because increasing the model width
ensures a large enough space of candidate permutations, increasing the chance
of finding one that yields LMC. In this work, we empirically demonstrate that,
even without any permutations, simply widening the models is sufficient for
achieving LMC when using a suitable softmax temperature calibration. We further
explain why this phenomenon arises by analyzing intermediate layer outputs.
Specifically, we introduce layerwise exponentially weighted connectivity
(LEWC), which states that the output of each layer of the merged model can be
represented as an exponentially weighted sum of the outputs of the
corresponding layers of the original models. Consequently the merged model's
output matches that of an ensemble of the original models, which facilitates
LMC. To the best of our knowledge, this work is the first to show that widening
the model not only facilitates nonlinear mode connectivity, as suggested in
prior research, but also significantly increases the possibility of achieving
linear mode connectivity.

</details>


### [57] [Mitigating Subject Dependency in EEG Decoding with Subject-Specific Low-Rank Adapters](https://arxiv.org/abs/2510.08059)
*Timon Klein,Piotr Minakowski,Sebastian Sager*

Main category: cs.LG

TL;DR: 提出Subject-Conditioned Layer来解决EEG解码中特定受试者分布偏移问题，通过分解权重为共享组件和个性化低秩校正，使现有模型对受试者偏移具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 特定受试者的分布偏移是EEG解码基础模型发展的重要障碍，需要解决跨受试者的适应性问题。

Method: 设计自适应层作为标准线性或卷积层的替代，将权重分解为共享的受试者不变组件和轻量级低秩校正，每个受试者拥有独特的校正部分。

Result: 配备该层的模型在性能上优于仅共享权重的模型和单独训练的受试者特定模型的平均值。

Conclusion: Subject-Conditioned Layer为构建有效的跨受试者EEG基础模型提供了实用且可扩展的路径。

Abstract: Subject-specific distribution shifts represent an important obstacle to the
development of foundation models for EEG decoding. To address this, we propose
Subject-Conditioned Layer,, an adaptive layer designed as a drop-in replacement
for standard linear or convolutional layers in any neural network architecture.
Our layer captures subject-specific variability by decomposing its weights into
a shared, subject-invariant component and a lightweight, low-rank correction
unique to each subject. This explicit separation of general knowledge from
personalized adaptation allows existing models to become robust to subject
shifts. Empirically, models equipped with our layer outperform both a
shared-weight-only model (subject-agnostic model) and the average of
individually trained subject-specific models. Consequently, the
Subject-Conditioned Layer, offers a practical and scalable path towards
building effective cross-subject foundation models for EEG.

</details>


### [58] [Bayesian Decision Making around Experts](https://arxiv.org/abs/2510.08113)
*Daniel Jarne Ornia,Joel Dyer,Nicholas Bishop,Anisoara Calinescu,Michael Wooldridge*

Main category: cs.LG

TL;DR: 本文研究学习智能体如何最优地整合专家数据，包括离线设置和同时设置，提出了信息理论框架来量化专家数据的价值，并设计了智能决策算法。


<details>
  <summary>Details</summary>
Motivation: 随着复杂学习智能体与现有专家（如人类操作员或已训练智能体）共同部署，需要解决如何最优整合与学习者自身经验结构不同的专家数据的问题。

Method: 在贝叶斯多臂老虎机背景下，研究两种设置：(i)离线设置：学习者在交互前接收专家最优策略的结果数据集；(ii)同时设置：学习者在每一步选择基于自身经验或专家同时获得的结果更新信念。提出信息导向规则，使学习者处理能最大化关于最优行动一步信息增益的数据源。

Result: 证明在专家结果上预训练通过专家数据与最优行动之间的互信息收紧信息理论遗憾界。提出了学习者推断何时信任专家、何时不信任的策略，以保护学习者免受专家无效或被破坏的情况。

Conclusion: 通过量化专家数据的价值，该框架为智能体智能决定何时向他人学习提供了实用的信息理论算法。

Abstract: Complex learning agents are increasingly deployed alongside existing experts,
such as human operators or previously trained agents. However, it remains
unclear how should learners optimally incorporate certain forms of expert data,
which may differ in structure from the learner's own action-outcome
experiences. We study this problem in the context of Bayesian multi-armed
bandits, considering: (i) offline settings, where the learner receives a
dataset of outcomes from the expert's optimal policy before interaction, and
(ii) simultaneous settings, where the learner must choose at each step whether
to update its beliefs based on its own experience, or based on the outcome
simultaneously achieved by an expert. We formalize how expert data influences
the learner's posterior, and prove that pretraining on expert outcomes tightens
information-theoretic regret bounds by the mutual information between the
expert data and the optimal action. For the simultaneous setting, we propose an
information-directed rule where the learner processes the data source that
maximizes their one-step information gain about the optimal action. Finally, we
propose strategies for how the learner can infer when to trust the expert and
when not to, safeguarding the learner for the cases where the expert is
ineffective or compromised. By quantifying the value of expert data, our
framework provides practical, information-theoretic algorithms for agents to
intelligently decide when to learn from others.

</details>


### [59] [Approximate Domain Unlearning for Vision-Language Models](https://arxiv.org/abs/2510.08132)
*Kodai Kawamura,Yuta Goto,Rintaro Yanagi,Hirokatsu Kataoka,Go Irie*

Main category: cs.LG

TL;DR: 本文提出了近似领域遗忘（ADU）的新问题设置，旨在从预训练视觉语言模型中移除特定领域的知识（如插图），同时保持对其他领域（如真实图像）的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有的近似遗忘方法主要关注类别遗忘，但在实际应用中仅遗忘对象类别往往不够。例如自动驾驶系统需要准确识别真实汽车，同时避免将路边广告中的插图汽车误认为真实汽车。

Method: 提出了一种新方法，通过显式解缠领域分布并自适应捕捉实例特定的领域信息来解决领域分布高度纠缠的问题。

Result: 大量实验表明，该方法在基于VLM调优技术构建的基线方法上表现更优。

Conclusion: 该方法为VLMs中实用和细粒度的遗忘铺平了道路。

Abstract: Pre-trained Vision-Language Models (VLMs) exhibit strong generalization
capabilities, enabling them to recognize a wide range of objects across diverse
domains without additional training. However, they often retain irrelevant
information beyond the requirements of specific downstream tasks, raising
concerns about computational efficiency and potential information leakage. This
has motivated growing interest in approximate unlearning, which aims to
selectively remove unnecessary knowledge while preserving overall model
performance. Existing approaches to approximate unlearning have primarily
focused on class unlearning, where a VLM is retrained to fail to recognize
specified object classes while maintaining accuracy for others. However, merely
forgetting object classes is often insufficient in practical applications. For
instance, an autonomous driving system should accurately recognize real cars
while avoiding misrecognition of illustrated cars depicted in roadside
advertisements as real cars, which could be hazardous. In this paper, we
introduce Approximate Domain Unlearning (ADU), a novel problem setting that
requires reducing recognition accuracy for images from specified domains (e.g.,
illustration) while preserving accuracy for other domains (e.g., real). ADU
presents new technical challenges: due to the strong domain generalization
capability of pre-trained VLMs, domain distributions are highly entangled in
the feature space, making naive approaches based on penalizing target domains
ineffective. To tackle this limitation, we propose a novel approach that
explicitly disentangles domain distributions and adaptively captures
instance-specific domain information. Extensive experiments show that our
approach outperforms baselines built upon VLM tuning techniques, paving the way
for practical and fine-grained unlearning in VLMs. Code:
https://kodaikawamura.github.io/Domain_Unlearning/.

</details>


### [60] [Arbitrary Entropy Policy Optimization: Entropy Is Controllable in Reinforcement Finetuning](https://arxiv.org/abs/2510.08141)
*Chen Wang,Zhaochun Li,Jionghao Bai,Yuzhi Zhang,Shisheng Cui,Zhou Zhao,Yue Wang*

Main category: cs.LG

TL;DR: 提出了AEPO方法解决强化学习微调中的熵崩溃问题，通过温度调节实现精确的熵控制，揭示了熵与性能的非单调关系，并提供了更广泛的RFT范式。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法存在熵崩溃问题，即熵单调下降导致探索消失和策略过早收敛，现有熵正则化方法只能部分缓解此问题但引入偏差和不稳定性。

Method: 提出AEPO方法，用REINFORCE策略梯度替代熵奖励，通过温度调节稳定熵，包含三个关键设计：策略梯度作为正则化、分布作为正则化、REINFORCE作为正则化。

Result: AEPO能稳定地将熵控制在任意目标水平，有效消除GRPO中的熵崩溃；揭示了熵与性能的非单调关系；提供了更广泛的RFT范式。

Conclusion: AEPO解决了熵控制问题，阐明了熵、探索和推理性能之间的关系，并为RFT提供了新的范式，其中优越的目标分布可作为REINFORCE正则化器。

Abstract: Reinforcement finetuning (RFT) is essential for enhancing the reasoning
capabilities of large language models (LLM), yet the widely adopted Group
Relative Policy Optimization (GRPO) suffers from entropy collapse, where
entropy monotonically decreases, exploration vanishes, and policies converge
prematurely. Existing entropy-regularized methods only partially alleviate this
issue while introducing bias and instability, leaving entropy control
unresolved and the connection between entropy, exploration, and performance
unclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which
eliminates entropy collapse by replacing entropy bonuses with REINFORCE policy
gradient on temperature-adjusted distributions and stabilizing entropy through
temperature regulation. AEPO integrates three key designs: policy gradient as
regularization, distribution as regularization, and REINFORCE as
regularization, enabling precise entropy control without distorting
optimization. Experiments demonstrate three major contributions: AEPO (1)
stabilizes entropy at arbitrary target levels, effectively removing collapse in
GRPO; (2) reveals a non-monotonic relation where performance first improves
then declines with increasing entropy, clarifying the link between entropy,
exploration, and reasoning; and (3) generalizes beyond entropy, providing a
broader RFT paradigm where superior target distributions can serve as REINFORCE
regularizers.

</details>


### [61] [Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning](https://arxiv.org/abs/2510.08146)
*Aman Sharma,Paras Chopra*

Main category: cs.LG

TL;DR: 提出基于熵的框架，利用token级logprobs的香农熵作为置信度信号实现早期停止，在保持任务准确性的同时节省25-50%计算成本。


<details>
  <summary>Details</summary>
Motivation: 利用推理模型中出现的置信度机制来提高token效率，减少计算延迟。

Method: 使用token级logprobs的香农熵作为置信度信号，通过熵阈值实现早期停止推理。

Result: 在推理优化模型系列中实现25-50%计算成本降低，同时保持准确性。

Conclusion: 基于熵的置信度校准是现代后训练推理系统的显著特征，可用于有效节省计算资源。

Abstract: We introduce a simple, yet novel entropy-based framework to drive token
efficiency in large language models during reasoning tasks. Our approach uses
Shannon entropy from token-level logprobs as a confidence signal to enable
early stopping, achieving 25-50% computational savings while maintaining task
accuracy. Crucially, we demonstrate that entropy-based confidence calibration
represents an emergent property of advanced post-training optimization present
in modern reasoning models but notably absent in standard instruction-tuned and
pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop
reasoning varies from model to model but can be calculated easily in one shot
using only a few examples from existing reasoning datasets. Our results
indicate that advanced reasoning models often know that they've gotten a
correct answer early on, and that this emergent confidence awareness can be
exploited to save tokens and reduce latency. The framework demonstrates
consistent performance across reasoning-optimized model families with 25-50%
computational cost reduction while preserving accuracy, revealing that
confidence mechanisms represent a distinguishing characteristic of modern
post-trained reasoning systems versus their predecessors.

</details>


### [62] [Unsupervised Multi-Source Federated Domain Adaptation under Domain Diversity through Group-Wise Discrepancy Minimization](https://arxiv.org/abs/2510.08150)
*Larissa Reichart,Cem Ata Baykara,Ali Burak Ünal,Mete Akgün,Harlin Lee*

Main category: cs.LG

TL;DR: GALA是一个可扩展的联邦无监督多源域自适应框架，通过组间差异最小化和温度控制加权策略，解决了多源域自适应中的计算复杂性和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式无监督多源域自适应方法假设源域数量较少，无法有效扩展到大量异构域，导致计算开销高或性能不稳定。

Method: 提出两个关键组件：1）组间差异最小化目标，无需二次计算即可近似全对域对齐；2）温度控制的基于质心的加权策略，根据与目标域的对齐程度动态优先处理源域。

Result: 在标准基准测试中达到竞争性或最先进的结果，在多样多源设置中显著优于先前方法，而其他方法无法收敛。

Conclusion: GALA框架能够在大规模异构源域上实现稳定且可并行化的训练，为多源域自适应提供了可扩展的解决方案。

Abstract: Unsupervised multi-source domain adaptation (UMDA) aims to learn models that
generalize to an unlabeled target domain by leveraging labeled data from
multiple, diverse source domains. While distributed UMDA methods address
privacy constraints by avoiding raw data sharing, existing approaches typically
assume a small number of sources and fail to scale effectively. Increasing the
number of heterogeneous domains often makes existing methods impractical,
leading to high computational overhead or unstable performance. We propose
GALA, a scalable and robust federated UMDA framework that introduces two key
components: (1) a novel inter-group discrepancy minimization objective that
efficiently approximates full pairwise domain alignment without quadratic
computation; and (2) a temperature-controlled, centroid-based weighting
strategy that dynamically prioritizes source domains based on alignment with
the target. Together, these components enable stable and parallelizable
training across large numbers of heterogeneous sources. To evaluate performance
in high-diversity scenarios, we introduce Digit-18, a new benchmark comprising
18 digit datasets with varied synthetic and real-world domain shifts. Extensive
experiments show that GALA consistently achieves competitive or
state-of-the-art results on standard benchmarks and significantly outperforms
prior methods in diverse multi-source settings where others fail to converge.

</details>


### [63] [Beyond Sub-6 GHz: Leveraging mmWave Wi-Fi for Gait-Based Person Identification](https://arxiv.org/abs/2510.08160)
*Nabeel Nisar Bhat,Maksim Karnaukh,Jakob Struye,Rafael Berkvens,Jeroen Famaey*

Main category: cs.LG

TL;DR: 本文首次比较了商用Wi-Fi在sub-6 GHz和毫米波频段用于人员识别的性能，发现毫米波在低采样率下也能实现高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注sub-6 GHz频段，而毫米波因其更精细的空间分辨率可能提供新的识别机会，但两种频段的比较优势尚未被探索。

Method: 使用商用Wi-Fi设备在室内环境中收集同步的sub-6 GHz和毫米波测量数据，采用相同的训练流程和模型配置，利用端到端深度学习和有效的背景减除技术。

Result: 在低采样率（10 Hz）下，毫米波Wi-Fi信号结合背景减除技术，在20个人的识别任务中达到了91.2%的准确率。

Conclusion: 毫米波Wi-Fi信号在人员识别方面具有显著优势，即使在低采样率下也能实现高精度识别，为基于步态的人员识别提供了新的可能性。

Abstract: Person identification plays a vital role in enabling intelligent,
personalized, and secure human-computer interaction. Recent research has
demonstrated the feasibility of leveraging Wi-Fi signals for passive person
identification using a person's unique gait pattern. Although most existing
work focuses on sub-6 GHz frequencies, the emergence of mmWave offers new
opportunities through its finer spatial resolution, though its comparative
advantages for person identification remain unexplored. This work presents the
first comparative study between sub-6 GHz and mmWave Wi-Fi signals for person
identification with commercial off-the-shelf (COTS) Wi-Fi, using a novel
dataset of synchronized measurements from the two frequency bands in an indoor
environment. To ensure a fair comparison, we apply identical training pipelines
and model configurations across both frequency bands. Leveraging end-to-end
deep learning, we show that even at low sampling rates (10 Hz), mmWave Wi-Fi
signals can achieve high identification accuracy (91.2% on 20 individuals) when
combined with effective background subtraction.

</details>


### [64] [Bidirectional Representations Augmented Autoregressive Biological Sequence Generation:Application in De Novo Peptide Sequencing](https://arxiv.org/abs/2510.08169)
*Xiang Zhang,Jiaqi Wei,Zijie Qiu,Sheng Xu,Zhi Jin,ZhiQiang Gao,Nanqing Dong,Siqi Sun*

Main category: cs.LG

TL;DR: 提出了一种混合自回归-非自回归框架，通过动态整合双向上下文信息来增强生物序列生成，在九物种肽段从头测序基准测试中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在生物序列生成中受限于单向性，无法捕捉关键的全局双向依赖关系；非自回归模型虽然能提供双向表示，但在生成连贯性和可扩展性方面存在挑战。

Method: 采用共享输入编码器和双解码器架构：非自回归解码器学习潜在双向生物特征，自回归解码器利用这些特征合成序列，通过跨解码器注意力模块实现特征查询和整合，配合重要性退火和梯度阻断的训练策略。

Result: 在九物种肽段从头测序基准测试中，模型显著超越了自回归和非自回归基线方法，成功融合了自回归的稳定性与非自回归的上下文感知能力。

Conclusion: 该研究推进了生物序列建模技术，为增强自回归模型的双向理解能力提供了一种新颖的架构范式，在复杂序列生成任务中表现出色。

Abstract: Autoregressive (AR) models, common in sequence generation, are limited in
many biological tasks such as de novo peptide sequencing and protein modeling
by their unidirectional nature, failing to capture crucial global bidirectional
token dependencies. Non-Autoregressive (NAR) models offer holistic,
bidirectional representations but face challenges with generative coherence and
scalability. To transcend this, we propose a hybrid framework enhancing AR
generation by dynamically integrating rich contextual information from
non-autoregressive mechanisms. Our approach couples a shared input encoder with
two decoders: a non-autoregressive one learning latent bidirectional biological
features, and an AR decoder synthesizing the biological sequence by leveraging
these bidirectional features. A novel cross-decoder attention module enables
the AR decoder to iteratively query and integrate these bidirectional features,
enriching its predictions. This synergy is cultivated via a tailored training
strategy with importance annealing for balanced objectives and cross-decoder
gradient blocking for stable, focused learning. Evaluations on a demanding
nine-species benchmark of de novo peptide sequencing show that our model
substantially surpasses AR and NAR baselines. It uniquely harmonizes AR
stability with NAR contextual awareness, delivering robust, superior
performance on diverse downstream data. This research advances biological
sequence modeling techniques and contributes a novel architectural paradigm for
augmenting AR models with enhanced bidirectional understanding for complex
sequence generation. Code is available at https://github.com/BEAM-Labs/denovo.

</details>


### [65] [Long-tailed Recognition with Model Rebalancing](https://arxiv.org/abs/2510.08177)
*Jiaan Luo,Feng Hong,Qiang Hu,Xiaofeng Cao,Feng Liu,Jiangchao Yao*

Main category: cs.LG

TL;DR: 本文提出了一种名为MORE（模型再平衡）的新框架，通过直接重新平衡模型的参数空间来解决长尾识别问题，而不增加整体模型复杂度或推理成本。


<details>
  <summary>Details</summary>
Motivation: 长尾识别在深度学习中普遍存在且具有挑战性，特别是在基础模型的下游微调中，因为偏斜的类别分布通常会阻碍模型对尾部类别的泛化能力。

Method: MORE引入了一个低秩参数组件，通过定制的损失函数和正弦重加权调度来调节参数空间分配，但不增加整体模型复杂度或推理成本。

Result: 在多样化的长尾基准测试（包括多类别和多标签任务）上的广泛实验表明，MORE显著提高了泛化能力，特别是对尾部类别，并能有效补充现有的不平衡缓解方法。

Conclusion: MORE有潜力作为长尾设置中的稳健即插即用模块。

Abstract: Long-tailed recognition is ubiquitous and challenging in deep learning and
even in the downstream finetuning of foundation models, since the skew class
distribution generally prevents the model generalization to the tail classes.
Despite the promise of previous methods from the perspectives of data
augmentation, loss rebalancing and decoupled training etc., consistent
improvement in the broad scenarios like multi-label long-tailed recognition is
difficult. In this study, we dive into the essential model capacity impact
under long-tailed context, and propose a novel framework, Model Rebalancing
(MORE), which mitigates imbalance by directly rebalancing the model's parameter
space. Specifically, MORE introduces a low-rank parameter component to mediate
the parameter space allocation guided by a tailored loss and sinusoidal
reweighting schedule, but without increasing the overall model complexity or
inference costs. Extensive experiments on diverse long-tailed benchmarks,
spanning multi-class and multi-label tasks, demonstrate that MORE significantly
improves generalization, particularly for tail classes, and effectively
complements existing imbalance mitigation methods. These results highlight
MORE's potential as a robust plug-and-play module in long-tailed settings.

</details>


### [66] [FuelCast: Benchmarking Tabular and Temporal Models for Ship Fuel Consumption](https://arxiv.org/abs/2510.08217)
*Justus Viga,Penelope Mueck,Alexander Löser,Torben Weis*

Main category: cs.LG

TL;DR: 本文介绍了船舶燃料消耗预测的新数据集和标准化基准，探索了TabPFN基础模型在船舶燃料建模中的应用，结果显示包含环境条件和时间上下文的模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 航运业中燃料消耗和排放对经济效率和环境可持续性有重要影响，但异构方法和有限的高质量数据集阻碍了建模方法的直接比较。

Method: 引入并发布包含三艘船舶运行和环境数据的新数据集；定义涵盖表格回归和时间序列回归的标准化基准；研究使用TabPFN基础模型进行船舶消耗建模的上下文学习方法。

Result: 所有评估模型都表现出强劲性能，支持船上数据驱动燃料预测的可行性。包含环境条件的模型始终优于仅依赖船速的简单多项式基线。TabPFN略微优于其他技术，包含时间上下文可提高准确性。

Conclusion: 基础模型具有上下文学习能力，在表格预测方面具有潜力，包含环境条件和时间上下文可显著提高船舶燃料消耗预测的准确性。

Abstract: In the shipping industry, fuel consumption and emissions are critical factors
due to their significant impact on economic efficiency and environmental
sustainability. Accurate prediction of ship fuel consumption is essential for
further optimization of maritime operations. However, heterogeneous
methodologies and limited high-quality datasets hinder direct comparison of
modeling approaches. This paper makes three key contributions: (1) we introduce
and release a new dataset
(https://huggingface.co/datasets/krohnedigital/FuelCast) comprising operational
and environmental data from three ships; (2) we define a standardized benchmark
covering tabular regression and time-series regression (3) we investigate the
application of in-context learning for ship consumption modeling using the
TabPFN foundation model - a first in this domain to our knowledge. Our results
demonstrate strong performance across all evaluated models, supporting the
feasibility of onboard, data-driven fuel prediction. Models incorporating
environmental conditions consistently outperform simple polynomial baselines
relying solely on vessel speed. TabPFN slightly outperforms other techniques,
highlighting the potential of foundation models with in-context learning
capabilities for tabular prediction. Furthermore, including temporal context
improves accuracy.

</details>


### [67] [Expressive Value Learning for Scalable Offline Reinforcement Learning](https://arxiv.org/abs/2510.08218)
*Nicolas Espinosa-Dice,Kiante Brantley,Wen Sun*

Main category: cs.LG

TL;DR: EVOR是一种可扩展的离线强化学习方法，通过流匹配学习正则化的最优Q函数，在推理时通过拒绝采样从表达性价值函数中提取策略，避免了蒸馏或反向传播时间依赖。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在机器人领域尚未充分发挥潜力，主要因为可扩展性问题。现有方法依赖计算昂贵的BPTT或引入复合误差的策略蒸馏，限制了向更复杂数据集的扩展。

Method: EVOR结合表达性策略和表达性价值函数，通过流匹配训练学习正则化的最优Q函数，在推理时使用拒绝采样进行策略提取，实现高效优化和计算可扩展搜索。

Result: 实验表明EVOR在多样化离线RL任务上优于基线方法，验证了将表达性价值学习整合到离线RL中的优势。

Conclusion: EVOR提供了一种无需蒸馏或BPTT的可扩展离线RL方法，通过表达性价值学习实现了更好的性能表现。

Abstract: Reinforcement learning (RL) is a powerful paradigm for learning to make
sequences of decisions. However, RL has yet to be fully leveraged in robotics,
principally due to its lack of scalability. Offline RL offers a promising
avenue by training agents on large, diverse datasets, avoiding the costly
real-world interactions of online RL. Scaling offline RL to increasingly
complex datasets requires expressive generative models such as diffusion and
flow matching. However, existing methods typically depend on either
backpropagation through time (BPTT), which is computationally prohibitive, or
policy distillation, which introduces compounding errors and limits scalability
to larger base policies. In this paper, we consider the question of how to
develop a scalable offline RL approach without relying on distillation or
backpropagation through time. We introduce Expressive Value Learning for
Offline Reinforcement Learning (EVOR): a scalable offline RL approach that
integrates both expressive policies and expressive value functions. EVOR learns
an optimal, regularized Q-function via flow matching during training. At
inference-time, EVOR performs inference-time policy extraction via rejection
sampling against the expressive value function, enabling efficient
optimization, regularization, and compute-scalable search without retraining.
Empirically, we show that EVOR outperforms baselines on a diverse set of
offline RL tasks, demonstrating the benefit of integrating expressive value
learning into offline RL.

</details>


### [68] [Post-hoc Stochastic Concept Bottleneck Models](https://arxiv.org/abs/2510.08219)
*Wiktor Jan Hoffmann,Sonia Laguna,Moritz Vandenhirtz,Emanuele Palumbo,Julia E. Vogt*

Main category: cs.LG

TL;DR: 提出了PSCBMs方法，通过添加小型协方差预测模块来增强预训练的CBM，无需重新训练主干模型，显著提高了概念干预下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有建模概念依赖性的方法通常需要重新训练整个模型，这在原始数据或计算资源有限的情况下不可行。

Method: 引入PSCBMs方法，通过添加协方差预测模块为预训练CBM建立概念的多变量正态分布，提出两种训练策略。

Result: 在真实世界数据上，PSCBMs在概念和目标准确性方面始终匹配或优于标准CBM，在概念干预下表现更好，且比从头训练类似随机模型更高效。

Conclusion: PSCBMs是一种轻量级方法，能够有效建模概念依赖性，提高干预性能，同时保持计算效率。

Abstract: Concept Bottleneck Models (CBMs) are interpretable models that predict the
target variable through high-level human-understandable concepts, allowing
users to intervene on mispredicted concepts to adjust the final output. While
recent work has shown that modeling dependencies between concepts can improve
CBM performance, especially under interventions, such approaches typically
require retraining the entire model, which may be infeasible when access to the
original data or compute is limited. In this paper, we introduce Post-hoc
Stochastic Concept Bottleneck Models (PSCBMs), a lightweight method that
augments any pre-trained CBM with a multivariate normal distribution over
concepts by adding only a small covariance-prediction module, without
retraining the backbone model. We propose two training strategies and show on
real-world data that PSCBMs consistently match or improve both concept and
target accuracy over standard CBMs at test time. Furthermore, we show that due
to the modeling of concept dependencies, PSCBMs perform much better than CBMs
under interventions, while remaining far more efficient than retraining a
similar stochastic model from scratch.

</details>


### [69] [Reinforcement Learning from Probabilistic Forecasts for Safe Decision-Making via Conditional Value-at-Risk Planning](https://arxiv.org/abs/2510.08226)
*Michal Koren,Or Peretz,Tai Dinh,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了不确定性感知马尔可夫决策过程（UAMDP），这是一个结合贝叶斯预测、后验采样强化学习和条件风险价值约束的统一框架，用于在波动性高风险环境中进行顺序决策。


<details>
  <summary>Details</summary>
Motivation: 在波动性高、风险大的顺序决策环境中，仅最大化期望收益是不够的，需要系统性的不确定性管理方法。

Method: UAMDP框架将贝叶斯预测、后验采样强化学习（Thompson采样）和条件风险价值（CVaR）约束下的规划相结合，在闭环中更新潜在动态的信念，采样可能的未来情景，并根据预设风险容忍度优化策略。

Result: 在高频股票交易和零售库存控制两个领域的评估中，UAMDP相比强深度学习基线显著提高了长期预测准确性（RMSE降低25%，sMAPE降低32%），交易夏普比率从1.54提升至1.74，最大回撤减半。

Conclusion: 整合校准的概率建模、与后验不确定性对齐的探索以及风险感知控制，能够产生稳健、可泛化的方法，实现更安全、更有利的顺序决策。

Abstract: Sequential decisions in volatile, high-stakes settings require more than
maximizing expected return; they require principled uncertainty management.
This paper presents the Uncertainty-Aware Markov Decision Process (UAMDP), a
unified framework that couples Bayesian forecasting, posterior-sampling
reinforcement learning, and planning under a conditional value-at-risk (CVaR)
constraint. In a closed loop, the agent updates its beliefs over latent
dynamics, samples plausible futures via Thompson sampling, and optimizes
policies subject to preset risk tolerances. We establish regret bounds that
converge to the Bayes-optimal benchmark under standard regularity conditions.
We evaluate UAMDP in two domains-high-frequency equity trading and retail
inventory control-both marked by structural uncertainty and economic
volatility. Relative to strong deep learning baselines, UAMDP improves
long-horizon forecasting accuracy (RMSE decreases by up to 25\% and sMAPE by
32\%), and these gains translate into economic performance: the trading Sharpe
ratio rises from 1.54 to 1.74 while maximum drawdown is roughly halved. These
results show that integrating calibrated probabilistic modeling, exploration
aligned with posterior uncertainty, and risk-aware control yields a robust,
generalizable approach to safer and more profitable sequential decision-making.

</details>


### [70] [Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization](https://arxiv.org/abs/2510.08233)
*Yuchen Zhu,Wei Guo,Jaemoo Choi,Petr Molodyk,Bo Yuan,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 提出了DMPO（分布匹配策略优化），一种专门为扩散大语言模型设计的强化学习微调方法，通过交叉熵优化匹配策略分布与最优奖励倾斜分布，显著提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型是自回归大语言模型的有前景替代方案，具有更高的推理吞吐量潜力，但需要专门的强化学习算法来实现与自回归模型相当的性能，特别是在推理任务上。

Method: 提出DMPO方法，通过分布匹配框架，使用交叉熵优化使dLLM策略分布与最优奖励倾斜分布对齐，并针对小训练批次大小挑战提出了新颖的权重基线减法技术。

Result: 在多个推理基准测试中表现出卓越性能，无需监督微调，准确率比之前SOTA基线提升高达42.9%，比基础模型提升55.8%。

Conclusion: DMPO证明了分布匹配框架的有效性，为扩散大语言模型的强化学习微调提供了理论基础和实践解决方案。

Abstract: Diffusion large language models (dLLMs) are promising alternatives to
autoregressive large language models (AR-LLMs), as they potentially allow
higher inference throughput. Reinforcement learning (RL) is a crucial component
for dLLMs to achieve comparable performance with AR-LLMs on important tasks,
such as reasoning. However, RL algorithms that are well-suited for dLLMs'
unique characteristics have yet to be developed. This paper proposes
Distribution Matching Policy Optimization (DMPO), a principled and
theoretically grounded RL fine-tuning method specifically designed to enhance
the reasoning capabilities of dLLMs by matching the dLLM policy distribution to
the optimal, reward-tilted one through cross-entropy optimization. We identify
a key challenge in the implementation with a small training batch size and
propose several effective solutions through a novel weight baseline subtraction
technique. DMPO exhibits superior performance on multiple reasoning benchmarks
without supervised fine-tuning, with an accuracy improvement of up to $42.9\%$
over previously SOTA baselines and $55.8\%$ over the base model, underscoring
the effectiveness of the distribution matching framework. Our code is available
at https://github.com/yuchen-zhu-zyc/DMPO.

</details>


### [71] [The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models](https://arxiv.org/abs/2510.08236)
*Konrad Löhr,Shuzhou Yuan,Michael Färber*

Main category: cs.LG

TL;DR: 该研究使用政治罗盘测试评估了8个主流大语言模型的政治偏见和刻板印象传播，发现所有模型都表现出左倾政治倾向，且通过语言变化引发的隐性刻板印象比显性角色提示更明显。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在信息传播和决策过程中日益重要，理解其政治偏见对于防止对公共舆论和民主进程产生不当影响至关重要。

Method: 使用二维政治罗盘测试评估模型固有政治倾向，通过角色提示探索显性刻板印象，使用多语言版本PCT揭示隐性刻板印象。

Result: 所有模型都表现出一致的左倾政治倾向；隐性刻板印象比显性更明显；大多数模型的隐性和显性刻板印象存在显著一致性。

Conclusion: 研究强调了大语言模型中政治偏见和刻板印象的复杂相互作用，表明模型对其固有偏见具有一定程度的透明度或"意识"。

Abstract: Large Language Models (LLMs) are increasingly integral to information
dissemination and decision-making processes. Given their growing societal
influence, understanding potential biases, particularly within the political
domain, is crucial to prevent undue influence on public opinion and democratic
processes. This work investigates political bias and stereotype propagation
across eight prominent LLMs using the two-dimensional Political Compass Test
(PCT). Initially, the PCT is employed to assess the inherent political leanings
of these models. Subsequently, persona prompting with the PCT is used to
explore explicit stereotypes across various social dimensions. In a final step,
implicit stereotypes are uncovered by evaluating models with multilingual
versions of the PCT. Key findings reveal a consistent left-leaning political
alignment across all investigated models. Furthermore, while the nature and
extent of stereotypes vary considerably between models, implicit stereotypes
elicited through language variation are more pronounced than those identified
via explicit persona prompting. Interestingly, for most models, implicit and
explicit stereotypes show a notable alignment, suggesting a degree of
transparency or "awareness" regarding their inherent biases. This study
underscores the complex interplay of political bias and stereotypes in LLMs.

</details>


### [72] [Opponent Shaping in LLM Agents](https://arxiv.org/abs/2510.08255)
*Marta Emili Garcia Segura,Stephen Hailes,Mirco Musolesi*

Main category: cs.LG

TL;DR: 本文首次研究了基于LLM的智能体在对手塑造（Opponent Shaping）方面的能力，提出了ShapeLLM方法，证明了LLM智能体能够在多种博弈环境中引导对手行为并影响集体福利。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为自主智能体在现实环境中部署，多智能体交互变得不可避免，需要理解LLM智能体是否能够像强化学习智能体一样通过交互来塑造对手的学习动态和行为。

Method: 提出了ShapeLLM方法，这是针对基于transformer的智能体量身定制的无模型对手塑造方法的适配版本，解决了现有OS算法无法直接应用于LLM的问题。

Result: 实验证明LLM智能体能够在竞争性博弈中成功引导对手走向可被利用的均衡，在合作性博弈中促进协调并改善集体福利，表明LLM智能体既能塑造也能被塑造。

Conclusion: LLM智能体能够通过交互塑造对手行为，确立了对手塑造作为多智能体LLM研究的关键维度。

Abstract: Large Language Models (LLMs) are increasingly being deployed as autonomous
agents in real-world environments. As these deployments scale, multi-agent
interactions become inevitable, making it essential to understand strategic
behavior in such systems. A central open question is whether LLM agents, like
reinforcement learning agents, can shape the learning dynamics and influence
the behavior of others through interaction alone. In this paper, we present the
first investigation of opponent shaping (OS) with LLM-based agents. Existing OS
algorithms cannot be directly applied to LLMs, as they require higher-order
derivatives, face scalability constraints, or depend on architectural
components that are absent in transformers. To address this gap, we introduce
ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based
agents. Using ShapeLLM, we examine whether LLM agents can influence co-players'
learning dynamics across diverse game-theoretic environments. We demonstrate
that LLM agents can successfully guide opponents toward exploitable equilibria
in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and
Chicken) and promote coordination and improve collective welfare in cooperative
games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma).
Our findings show that LLM agents can both shape and be shaped through
interaction, establishing opponent shaping as a key dimension of multi-agent
LLM research.

</details>


### [73] [Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization](https://arxiv.org/abs/2510.08256)
*Jason Bohne,Pawel Polak,David Rosenberg,Brian Bloniarz,Gary Kazantsev*

Main category: cs.LG

TL;DR: Mix-和MoE-DPO框架通过引入软混合模型和专家混合架构扩展了DPO，使用随机变分推理方法学习专业化专家策略，在多样偏好分布下提供更好的表达能力和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法依赖单一模型，限制了在多任务设置中的表达能力以及对异构或多样偏好分布的适应性。

Method: 提出基于随机变分推理的框架，引入专家分配潜变量模型，优化变分证据下界，支持共享基础架构和独立专家模型。

Result: 在各种模型大小和多偏好数据集上验证，证明Mix-和MoE-DPO为基于偏好的LLM对齐提供了强大且可扩展的方法。

Conclusion: Mix-和MoE-DPO通过混合建模提供了三个关键优势：通用函数逼近、奖励和策略专业化、上下文对齐，是DPO的有效扩展。

Abstract: Direct Preference Optimization (DPO) has recently emerged as a simple and
effective alternative to reinforcement learning from human feedback (RLHF) for
aligning large language models (LLMs) with user preferences. However, existing
DPO formulations rely on a single monolithic model, which limits their
expressivity in multi-task settings and their adaptability to heterogeneous or
diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a
framework that extends DPO with both soft mixture models and mixture-of-experts
(MoE) architectures, using a stochastic variational inference approach. Our
method introduces a latent-variable model over expert assignments and optimizes
a variational evidence lower bound (ELBO), enabling stable and efficient
learning of specialized expert policies from preference data. Mix- and MoE-DPO
provides three key advantages over standard DPO: (i) generalization via
universal function approximation through mixtures; (ii) reward and policy
specialization through expert components tailored to distinct preference modes;
and (iii) contextual alignment through input-dependent soft gating that enables
user-specific mixture policies. Our framework supports both shared base
architectures with expert-specific policy heads and fully independent expert
models, allowing flexible trade-offs between parameter efficiency and
specialization. We validate our approach on a variety of model sizes and
multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a
powerful and scalable method for preference-based LLM alignment.

</details>


### [74] [Counterfactual Identifiability via Dynamic Optimal Transport](https://arxiv.org/abs/2510.08294)
*Fabio De Sousa Ribeiro,Ainkaran Santhirasekaram,Ben Glocker*

Main category: cs.LG

TL;DR: 本文解决了从观测数据中识别高维多元结果反事实的开放性问题，建立了基于连续时间流的多元反事实识别基础，确保因果推断的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实推断方法缺乏识别性，削弱了其因果估计的有效性。Pearl(2000)强调反事实必须可识别才能证明因果主张的合理性。

Method: 使用连续时间流匹配方法，包括非马尔可夫设置，利用动态最优传输工具构建唯一、单调且保秩的反事实传输映射。

Result: 在具有反事实真实值的受控场景中验证了理论，并在真实图像上展示了公理化反事实合理性的改进。

Conclusion: 建立了多元反事实识别的理论基础，确保反事实推断的一致性和因果有效性，为高维观测数据的因果分析提供了可靠方法。

Abstract: We address the open question of counterfactual identification for
high-dimensional multivariate outcomes from observational data. Pearl (2000)
argues that counterfactuals must be identifiable (i.e., recoverable from the
observed data distribution) to justify causal claims. A recent line of work on
counterfactual inference shows promising results but lacks identification,
undermining the causal validity of its estimates. To address this, we establish
a foundation for multivariate counterfactual identification using
continuous-time flows, including non-Markovian settings under standard
criteria. We characterise the conditions under which flow matching yields a
unique, monotone and rank-preserving counterfactual transport map with tools
from dynamic optimal transport, ensuring consistent inference. Building on
this, we validate the theory in controlled scenarios with counterfactual
ground-truth and demonstrate improvements in axiomatic counterfactual soundness
on real images.

</details>


### [75] [Bridging the Physics-Data Gap with FNO-Guided Conditional Flow Matching: Designing Inductive Bias through Hierarchical Physical Constraints](https://arxiv.org/abs/2510.08295)
*Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 提出了一种将物理定律层次结构嵌入深度生成模型的分层框架，通过傅里叶神经算子和条件流匹配相结合，实现物理约束的时间序列生成。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列生成方法往往忽略领域特定的物理约束，限制了统计和物理一致性，需要将物理定律的固有层次结构直接嵌入生成模型。

Method: 结合傅里叶神经算子学习物理算子，使用条件流匹配进行概率生成，通过时间依赖的分层约束和FNO引导的修正进行集成。

Result: 在谐振子、人类活动识别和锂离子电池退化实验中，相比基线方法生成质量提高16.3%，物理违规减少46%，预测准确率提升18.5%。

Conclusion: 该框架成功将物理信息归纳偏置引入生成模型，显著提升了生成质量、物理一致性和预测性能。

Abstract: Conventional time-series generation often ignores domain-specific physical
constraints, limiting statistical and physical consistency. We propose a
hierarchical framework that embeds the inherent hierarchy of physical
laws-conservation, dynamics, boundary, and empirical relations-directly into
deep generative models, introducing a new paradigm of physics-informed
inductive bias. Our method combines Fourier Neural Operators (FNOs) for
learning physical operators with Conditional Flow Matching (CFM) for
probabilistic generation, integrated via time-dependent hierarchical
constraints and FNO-guided corrections. Experiments on harmonic oscillators,
human activity recognition, and lithium-ion battery degradation show 16.3%
higher generation quality, 46% fewer physics violations, and 18.5% improved
predictive accuracy over baselines.

</details>


### [76] [Learning What's Missing: Attention Dispersion and EMA Stabilization in Length Generalization](https://arxiv.org/abs/2510.08341)
*Pál Zsámboki,Benjamin Levi,David Ansel Josef Smith,Mitansh Kagalwala,Arlington Kell,Samuel Liechty,Cong Wang*

Main category: cs.LG

TL;DR: 本文研究了Transformer在集合补集任务中的长度泛化能力，证明了单层注意力Transformer的嵌入和值维度边界，并发现当模型在长度1和2上实现平衡logit位移时，可以泛化到更长的序列。研究还揭示了softmax压缩和训练噪声是限制泛化的两个机制，并提出dropout和EMA可以改善这些问题。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在集合补集任务中的长度泛化能力，这对于棋盘游戏式推理至关重要，因为模型需要预测输入序列中缺失的token的均匀分布。

Method: 通过理论分析建立单层注意力Transformer的维度边界，证明平衡logit位移条件可以保证长度泛化。通过机制分析识别softmax压缩和训练噪声问题，提出使用dropout和EMA来改善。在集合补集任务和OthelloGPT上进行实验验证。

Result: 理论证明：单层注意力Transformer在满足平衡logit位移条件时可以长度泛化，但精度会降低。实验验证：dropout可以对抗softmax压缩效应，EMA可以减轻训练噪声，两者都能改善长度泛化。在更复杂的OthelloGPT设置中，EMA同样有效。

Conclusion: Transformer的长度泛化受到softmax压缩和训练噪声的限制，但可以通过dropout和EMA等技术来改善。理论分析为理解这些机制提供了基础，实验验证了这些改进策略的有效性。

Abstract: We study length generalization in transformers through the set complement
task, where a model must predict a uniform distribution over tokens absent from
an input sequence -- an ability central to board-game style reasoning. Our main
theoretical result establishes two statements. First, we prove tight bounds on
embedding and value dimensions for single-layer attention-only transformers.
Second, we show that if such a model achieves balanced logit displacement at
lengths 1 and 2, then it must generalize to longer sequences, though with
reduced precision. A mechanistic reading of the proof explains this limitation:
as more tokens are attended to, softmax compresses logit displacements, eroding
separation between valid and invalid outputs. Training dynamics also suggest a
second obstacle: when many next tokens are possible, updates become noisy. We
hypothesize that dropout can counteract the first effect and Exponential Moving
Average (EMA) the second. We validate these hypotheses through random
hyperparameter search on the set complement task, which confirms both
mechanisms. We then test OthelloGPT, a GPT-1 style model trained on random
Othello moves, and find that EMA again improves length generalization in this
more complex setting.

</details>


### [77] [DeepEN: Personalized Enteral Nutrition for Critically Ill Patients using Deep Reinforcement Learning](https://arxiv.org/abs/2510.08350)
*Daniel Jason Tan,Jiayang Chen,Dilruk Perera,Kay Choong See,Mengling Feng*

Main category: cs.LG

TL;DR: DeepEN是一个基于深度强化学习的肠内营养个性化推荐框架，针对ICU重症患者，通过离线训练生成4小时一次的热量、蛋白质和液体摄入建议，相比临床指南方法可降低3.7个百分点的估计死亡率。


<details>
  <summary>Details</summary>
Motivation: 传统肠内营养治疗主要依赖指南或启发式方法，缺乏对患者个体生理状态动态变化的适应性。需要开发能够根据患者实时生理状况进行个性化营养推荐的数据驱动方法。

Method: 使用MIMIC-IV数据库中11000多名ICU患者数据进行离线训练，采用精心设计的临床信息状态空间和定制奖励函数，结合dueling double deep Q-network与保守Q学习正则化，学习与高价值临床行动一致的安全策略。

Result: DeepEN在各项定性和定量指标上均优于临床医生和指南策略，估计死亡率从22.5%降至18.8%（降低3.7±0.17个百分点），关键营养生物标志物也有改善。

Conclusion: 研究表明，安全的数据驱动肠内营养个性化治疗有潜力超越传统指南或启发式方法，改善患者预后。

Abstract: We introduce DeepEN, a deep reinforcement learning (RL) framework for
personalized enteral nutrition (EN) in critically ill patients. Trained offline
on over 11,000 ICU patients from the MIMIC-IV database, DeepEN generates
4-hourly recommendations for caloric, protein, and fluid intake tailored to
each patient's evolving physiology. The model integrates a curated, clinically
informed state space with a custom reward function that balances short-term
physiological and nutrition-related goals with long-term survival outcomes.
Using a dueling double deep Q-network with conservative Q-learning
regularization, DeepEN learns clinically realistic policies that align with
high-value clinician actions while discouraging unsafe deviations. Across
various qualitative and quantitative metrics, DeepEN outperforms
clinician-derived and guideline-based policies, achieving a 3.7 $\pm$ 0.17
percentage-point reduction in estimated mortality (18.8% vs 22.5%) and
improvements in key nutritional biomarkers. These findings highlight the
potential of safe, data-driven personalization of EN therapy to improve
outcomes beyond traditional guideline- or heuristic-based approaches.

</details>


### [78] [Guided Star-Shaped Masked Diffusion](https://arxiv.org/abs/2510.08369)
*Viacheslav Meshchaninov,Egor Shibaev,Artem Makoian,Ivan Klimov,Danil Sheshenya,Andrei Malinin,Nikita Balagansky,Daniil Gavrilov,Aibek Alanov,Dmitry Vetrov*

Main category: cs.LG

TL;DR: 提出了一种新的采样算法，通过轻量级微调单层和可学习的重掩码调度器，显著提升预训练掩码扩散模型的采样质量和效率，特别在少步采样场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 预训练掩码扩散模型的性能受限于其采样过程，该过程决策不可逆且在少步生成场景中表现不佳，需要改进采样算法以提升质量和效率。

Method: 采用星形范式重新构建生成过程，引入可学习的重掩码调度器智能识别和修正可能错误，仅需对单层进行轻量级微调。

Result: 在文本和代码生成任务上的综合实验表明，该采样算法优于或匹配现有方法，特别是在少步采样场景下带来显著质量提升。

Conclusion: 所提出的采样算法有效解决了预训练掩码扩散模型在采样过程中的局限性，通过可逆决策和错误修正机制显著提升了生成质量和效率。

Abstract: The performance of pre-trained masked diffusion models is often constrained
by their sampling procedure, which makes decisions irreversible and struggles
in low-step generation regimes. We introduce a novel sampling algorithm that
works with pre-trained models and, after a lightweight fine-tuning of a single
layer, significantly improves sample quality and efficiency. Our method
reformulates the generation process using a star-shaped paradigm, which
inherently allows for error correction. To make this process effective, we
augment it with a learnable re-masking scheduler that intelligently identifies
and revises likely errors. This approach yields a substantial quality boost,
particularly when using a small number of sampling steps. We extensively ablate
key components of our approach and show its usability in different scenarios.
In comprehensive experiments on text, and code generation, our sampling
algorithm outperforms or matches existing methods.

</details>


### [79] [Characterizing the Multiclass Learnability of Forgiving 0-1 Loss Functions](https://arxiv.org/abs/2510.08382)
*Jacob Trauger,Tyson Trauger,Ambuj Tewari*

Main category: cs.LG

TL;DR: 本文提出了一个基于Natarajan维度的新组合维度，用于刻画有限标签多类设置中可原谅0-1损失函数的学习能力，并建立了与集合值反馈学习的联系。


<details>
  <summary>Details</summary>
Motivation: 研究有限标签多类设置中可原谅0-1损失函数的学习能力特征化问题，填补现有理论空白。

Method: 创建基于Natarajan维度的广义Natarajan维度，并证明假设类在该设置下可学习当且仅当该维度有限。

Result: 证明了假设类在可原谅0-1损失函数设置下可学习当且仅当广义Natarajan维度有限，并建立了与集合值反馈学习的联系。

Conclusion: 集合学习问题的可学习性由Natarajan维度完全刻画，为多类学习理论提供了新的理论框架。

Abstract: In this paper we will give a characterization of the learnability of
forgiving 0-1 loss functions in the finite label multiclass setting. To do
this, we create a new combinatorial dimension that is based off of the
Natarajan Dimension \citep{natarajan1989learning} and we show that a hypothesis
class is learnable in our setting if and only if this Generalized Natarajan
Dimension is finite. We also show a connection to learning with set-valued
feedback. Through our results we show that the learnability of a set learning
problem is characterized by the Natarajan Dimension.

</details>


### [80] [FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts](https://arxiv.org/abs/2510.08396)
*Heming Zou,Yunliang Zang,Wutong Xu,Yao Zhu,Xiangyang Ji*

Main category: cs.LG

TL;DR: FlyLoRA是一种基于苍蝇嗅觉电路启发的LoRA变体，通过隐式MoE设计消除显式路由器，在参数高效微调中同时解决任务内去相关和任务间干扰问题。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法存在参数干扰问题，MoE-based LoRA变体虽然能缓解单任务内的相关性，但在多任务模型合并中仍存在任务间干扰，且需要额外路由器参数。

Method: 提出FlyLoRA：1）在上投影矩阵中引入按秩的专家激活；2）使用隐式路由器统一专家路由和下投影，用冻结的稀疏随机投影矩阵替代传统的密集可训练版本。

Result: 在通用知识理解、科学问答、数学推理和代码生成四个领域的广泛实验显示，相比现有方法获得了持续的性能提升。

Conclusion: FlyLoRA不仅取得了实证收益，还展示了生物结构如何启发AI技术创新，其随机矩阵的正交性特性天然缓解了任务间干扰。

Abstract: Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method for foundation models, but it suffers from parameter interference,
resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based
LoRA variants show promise in mitigating intra-task correlations in single-task
instruction tuning, they introduce additional router parameters and remain
ineffective in multi-task model merging where inter-task interference arises.
Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit
MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the
up-projection matrix, and (2) an implicit router that unifies expert routing
and down-projection, where a frozen sparse random projection matrix replaces
the traditional dense trainable version. This design resolves the trade-off
between intra-task decorrelation and computational efficiency by eliminating
the need for an explicit router, while inherently mitigating inter-task
interference due to the orthogonality property of random matrices. Extensive
experiments across four domains -- general knowledge understanding, scientific
question answering, mathematical reasoning, and code generation -- demonstrate
consistent performance improvements over existing methods. Beyond empirical
gains, FlyLoRA highlights how biological structures can inspire innovations in
AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.

</details>


### [81] [Prompts Generalize with Low Data: Non-vacuous Generalization Bounds for Optimizing Prompts with More Informative Priors](https://arxiv.org/abs/2510.08413)
*David Madras,Joshua Safyan,Qiuyi,Zhang*

Main category: cs.LG

TL;DR: 本文提出了基于困惑度正则化的提示优化方法，通过数据依赖的困惑度作为有效先验来改进提示泛化性能，并在数据稀缺场景下推导出非空泛的泛化界。


<details>
  <summary>Details</summary>
Motivation: 现有基于PAC-Bayes理论的提示优化方法在数据丰富时有效，但在数据稀缺时泛化界变得空泛。作者认为提示优化的成功可以通过更仔细地考虑数据或分布依赖的困惑度来解释，困惑度作为有效先验能够引导优化朝向更"自然"的提示。

Method: 提出了基于困惑度正则化的提示优化方法，通过困惑度限制探索空间来收紧泛化界。推导了在数据稀缺场景下非空泛的泛化界，并实证分析了困惑度正则化对提示泛化的实际益处。

Result: 理论分析表明困惑度正则化能够通过限制探索来收紧泛化界。实证研究验证了所提出泛化界的有效性以及困惑度正则化在改善提示泛化方面的实际优势。

Conclusion: 困惑度作为有效先验能够显著改善提示优化在数据稀缺场景下的泛化性能，提出的理论框架为理解提示工程的成功提供了更全面的解释。

Abstract: Many prompt engineering techniques have been successful in practice, even
when optimizing over a large prompt space with with a small amount of
task-specific data. Recent work has partially explained this success by showing
generalization bounds which apply PAC-Bayes theory to the discrete prompt
space, but they are non-vacuous only in data-rich scenarios. We argue that such
widespread success can be more fully explained through more carefully
considering data- or distribution-dependent perplexity, which acts as an
effective prior and steers the optimization towards prompts that are more
``natural'' for the task at hand. We derive novel generalization bounds that
are non-vacuous for data-scarce prompt optimization via more useful priors,
formally analyzing how perplexity regularization tightens these bounds by
limiting exploration. Empirically, we explore both the bounds' effectiveness
and the practical benefits of perplexity regularization in improving prompt
generalization.

</details>


### [82] [Reinforcing Diffusion Models by Direct Group Preference Optimization](https://arxiv.org/abs/2510.08425)
*Yihong Luo,Tianyang Hu,Jing Tang*

Main category: cs.LG

TL;DR: 提出了Direct Group Preference Optimization (DGPO)方法，这是一种新的在线强化学习算法，直接利用组级偏好信息，无需依赖低效的随机策略，从而可以使用高效的确定性ODE采样器，训练速度比现有方法快约20倍。


<details>
  <summary>Details</summary>
Motivation: 虽然GRPO等强化学习方法显著提升了大型语言模型，但将其适应到扩散模型仍具挑战性。GRPO需要随机策略，但最经济高效的扩散采样器基于确定性ODE。现有方法使用低效的SDE采样器引入随机性，但依赖模型无关的高斯噪声导致收敛缓慢。

Method: 提出DGPO算法，完全摒弃策略梯度框架，直接从组级偏好中学习，利用组内样本的相对信息。这种设计消除了对低效随机策略的需求，解锁了高效确定性ODE采样器的使用，实现更快的训练。

Result: 广泛实验结果显示，DGPO训练速度比现有最先进方法快约20倍，在领域内和领域外奖励指标上均取得优越性能。

Conclusion: DGPO通过直接学习组级偏好，成功解决了扩散模型中强化学习应用的效率问题，实现了显著的速度提升和性能改进。

Abstract: While reinforcement learning methods such as Group Relative Preference
Optimization (GRPO) have significantly enhanced Large Language Models, adapting
them to diffusion models remains challenging. In particular, GRPO demands a
stochastic policy, yet the most cost-effective diffusion samplers are based on
deterministic ODEs. Recent work addresses this issue by using inefficient
SDE-based samplers to induce stochasticity, but this reliance on model-agnostic
Gaussian noise leads to slow convergence. To resolve this conflict, we propose
Direct Group Preference Optimization (DGPO), a new online RL algorithm that
dispenses with the policy-gradient framework entirely. DGPO learns directly
from group-level preferences, which utilize relative information of samples
within groups. This design eliminates the need for inefficient stochastic
policies, unlocking the use of efficient deterministic ODE samplers and faster
training. Extensive results show that DGPO trains around 20 times faster than
existing state-of-the-art methods and achieves superior performance on both
in-domain and out-of-domain reward metrics. Code is available at
https://github.com/Luo-Yihong/DGPO.

</details>


### [83] [ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy Reinsurance Pricing](https://arxiv.org/abs/2510.08429)
*Stella C. Dong,James R. Finlay*

Main category: cs.LG

TL;DR: ClauseLens是一个基于条款的强化学习框架，用于生成透明、合规且风险感知的再保险条约报价，显著减少偿付能力违规并提高尾部风险表现。


<details>
  <summary>Details</summary>
Motivation: 当前再保险条约定价实践不透明且难以审计，需要满足严格的监管标准。

Method: 将报价任务建模为风险感知约束马尔可夫决策过程（RA-CMDP），从法律和承保语料库中检索法定和政策条款，嵌入到智能体观察中，用于约束可行行动并生成基于条款的自然语言解释。

Result: 在多智能体条约模拟器中评估，ClauseLens减少偿付能力违规51%，提高尾部风险表现27.9%（CVaR_0.10），条款基础解释准确率达88.2%，检索精度87.4%，召回率91.1%。

Conclusion: 将法律背景嵌入决策和解释路径可以产生可解释、可审计且符合监管要求的报价行为，与Solvency II、NAIC RBC和欧盟AI法案保持一致。

Abstract: Reinsurance treaty pricing must satisfy stringent regulatory standards, yet
current quoting practices remain opaque and difficult to audit. We introduce
ClauseLens, a clause-grounded reinforcement learning framework that produces
transparent, regulation-compliant, and risk-aware treaty quotes.
  ClauseLens models the quoting task as a Risk-Aware Constrained Markov
Decision Process (RA-CMDP). Statutory and policy clauses are retrieved from
legal and underwriting corpora, embedded into the agent's observations, and
used both to constrain feasible actions and to generate clause-grounded natural
language justifications.
  Evaluated in a multi-agent treaty simulator calibrated to industry data,
ClauseLens reduces solvency violations by 51%, improves tail-risk performance
by 27.9% (CVaR_0.10), and achieves 88.2% accuracy in clause-grounded
explanations with retrieval precision of 87.4% and recall of 91.1%.
  These findings demonstrate that embedding legal context into both decision
and explanation pathways yields interpretable, auditable, and
regulation-aligned quoting behavior consistent with Solvency II, NAIC RBC, and
the EU AI Act.

</details>


### [84] [xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning](https://arxiv.org/abs/2510.08439)
*Cheng Qian,Zuxin Liu,Shirley Kokane,Akshara Prabhakar,Jielin Qiu,Haolin Chen,Zhiwei Liu,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.LG

TL;DR: xRouter是一个基于工具调用的路由系统，通过强化学习训练智能路由器，在成本与性能之间实现最优平衡，无需手动设计路由规则。


<details>
  <summary>Details</summary>
Motivation: 现代LLM部署面临成本与性能的权衡：高端模型性能强但昂贵，轻量级模型经济但处理复杂任务能力弱。静态升级规则和关键词启发式方法无法充分利用这一频谱，也无法跨任务类型自适应。

Method: 采用基于工具调用的路由系统，训练一个学习型路由器，可以直接回答或调用外部模型。使用强化学习进行端到端训练，采用明确、成本感知的奖励函数来编码成本-性能权衡。

Result: 在多样化基准测试中，xRouter实现了强大的成本-性能权衡（例如在保持相似任务完成率的同时大幅降低成本），并为学习路由提供了实证见解。

Conclusion: xRouter为推进学习型、成本感知的LLM编排提供了实用基础，展示了学习路由的可行性和有效性。

Abstract: Modern LLM deployments confront a widening cost-performance spectrum: premium
models deliver strong reasoning but are expensive, while lightweight models are
economical yet brittle on complex tasks. Static escalation rules and keyword
heuristics under-utilize this spectrum and fail to adapt across task types. We
present xRouter, a tool-calling-based routing system in which a learned router
can either answer directly or invoke one or more external models. The router is
trained end-to-end with reinforcement learning using an explicit, cost-aware
reward that encodes cost-performance trade-offs, eliminating the need for
hand-engineered routing rules. Our implementation encompasses the full
reinforcement learning framework, including reward and cost accounting, as well
as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter
achieves strong cost-performance trade-offs (e.g., substantial cost reductions
at comparable task completion rates), and provides empirical insights into what
reliably helps learned routing and what does not, ranging from model
trainability to the difficulty of eliciting sophisticated orchestration
behaviors in small open models. We hope these findings and our open
implementation will serve as a practical substrate for advancing learned,
cost-aware LLM orchestration.

</details>


### [85] [Synthetic Series-Symbol Data Generation for Time Series Foundation Models](https://arxiv.org/abs/2510.08445)
*Wenxuan Wang,Kai Wu,Yujian Betterest Li,Dan Wang,Xiaoyu Zhang*

Main category: cs.LG

TL;DR: 该论文提出了SymTime，一种基于符号信息增强时间序列表示的预训练基础模型，通过系列-符号数据生成机制解决训练数据稀缺和不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析基础模型面临训练数据稀缺和不平衡的挑战，受到复杂动态系统理论的启发，需要开发能够无限制生成高质量时间序列数据的方法。

Method: 设计系列-符号数据生成机制，创建时间序列数据与对应符号表达式的配对；开发SymTime预训练基础模型，利用强相关的系列-符号数据对来增强时间序列表示。

Result: SymTime在五个主要时间序列分析任务中表现出色，与基于真实世界数据集预训练的基础模型相媲美。

Conclusion: 系列-符号数据生成和预训练机制在克服数据稀缺性和提升任务性能方面具有巨大潜力。

Abstract: Foundation models for time series analysis (TSA) have attracted significant
attention. However, challenges such as training data scarcity and imbalance
continue to hinder their development. Inspired by complex dynamic system
theories, we design a series-symbol data generation mechanism, enabling the
unrestricted creation of high-quality time series data paired with
corresponding symbolic expressions. To leverage series-symbol data pairs with
strong correlations, we develop \texttt{SymTime}, a pre-trained foundation
model for enhancing time series representation using symbolic information.
\texttt{SymTime} demonstrates competitive performance across five major TSA
tasks when fine-tunes with downstream tasks, rivaling foundation models
pre-trained on real-world datasets. This approach underscores the potential of
series-symbol data generation and pretraining mechanisms in overcoming data
scarcity and enhancing task performance. The code is available at
https://github.com/wwhenxuan/SymTime.

</details>


### [86] [Integral Signatures of Activation Functions: A 9-Dimensional Taxonomy and Stability Theory for Deep Learning](https://arxiv.org/abs/2510.08456)
*Ankur Mali,Lawrence Hall,Jake Williams,Gordon Richards*

Main category: cs.LG

TL;DR: 提出了一个基于九维积分签名的激活函数分类框架，结合高斯传播统计、渐近斜率和正则性度量，建立了激活函数的严格分类体系，并应用于八种标准激活函数的分析。


<details>
  <summary>Details</summary>
Motivation: 现有激活函数比较方法大多基于启发式，缺乏严格的分类框架。需要建立系统性的理论框架来理解激活函数的表达能力、稳定性和动态特性。

Method: 提出九维积分签名S_sigma(phi)，包含高斯传播统计(m1, g1, g2, m2, eta)、渐近斜率(alpha_plus, alpha_minus)和正则性度量(TV(phi'), C(phi))。通过动态分析、Lyapunov定理和核视角进行理论分析。

Result: 建立了激活函数的适定性、仿射重参数化定律和有界斜率变化下的闭包性质。推导了显式下降常数的Lyapunov定理和方差稳定性区域。对八种标准激活函数进行了分类，揭示了饱和型、线性增长型和平滑型激活函数之间的显著区别。

Conclusion: 该框架为激活函数选择提供了原则性设计指导，使激活函数选择从试错方法转向可证明的稳定性和核条件化方法。

Abstract: Activation functions govern the expressivity and stability of neural
networks, yet existing comparisons remain largely heuristic. We propose a
rigorous framework for their classification via a nine-dimensional integral
signature S_sigma(phi), combining Gaussian propagation statistics (m1, g1, g2,
m2, eta), asymptotic slopes (alpha_plus, alpha_minus), and regularity measures
(TV(phi'), C(phi)). This taxonomy establishes well-posedness, affine
reparameterization laws with bias, and closure under bounded slope variation.
Dynamical analysis yields Lyapunov theorems with explicit descent constants and
identifies variance stability regions through (m2', g2). From a kernel
perspective, we derive dimension-free Hessian bounds and connect smoothness to
bounded variation of phi'. Applying the framework, we classify eight standard
activations (ReLU, leaky-ReLU, tanh, sigmoid, Swish, GELU, Mish, TeLU), proving
sharp distinctions between saturating, linear-growth, and smooth families.
Numerical Gauss-Hermite and Monte Carlo validation confirms theoretical
predictions. Our framework provides principled design guidance, moving
activation choice from trial-and-error to provable stability and kernel
conditioning.

</details>


### [87] [SummDiff: Generative Modeling of Video Summarization with Diffusion](https://arxiv.org/abs/2510.08458)
*Kwanseok Kim,Jaehoon Hahm,Sumin Kim,Jinhwan Sul,Byunghak Kim,Joonseok Lee*

Main category: cs.LG

TL;DR: 本文提出SummDiff方法，将视频摘要重新定义为条件生成任务，首次在视频摘要中采用扩散模型，能够生成多个反映不同人类视角的合理摘要。


<details>
  <summary>Details</summary>
Motivation: 传统视频摘要方法忽略了任务固有的主观性，将多个评分者的评分平均化处理，无法反映不同人类对什么是好摘要的多样化观点。

Method: 提出SummDiff方法，将视频摘要构建为条件生成任务，采用扩散模型动态适应视觉上下文，根据输入视频生成多个候选摘要。

Result: SummDiff在多个基准测试中达到最先进性能，生成的摘要更贴近个体标注者偏好，并通过新颖的背包问题分析指标提供了更深入的见解。

Conclusion: 将视频摘要重新定义为条件生成任务，结合扩散模型能够有效捕捉任务的主观性，生成更符合人类多样化偏好的摘要。

Abstract: Video summarization is a task of shortening a video by choosing a subset of
frames while preserving its essential moments. Despite the innate subjectivity
of the task, previous works have deterministically regressed to an averaged
frame score over multiple raters, ignoring the inherent subjectivity of what
constitutes a good summary. We propose a novel problem formulation by framing
video summarization as a conditional generation task, allowing a model to learn
the distribution of good summaries and to generate multiple plausible summaries
that better reflect varying human perspectives. Adopting diffusion models for
the first time in video summarization, our proposed method, SummDiff,
dynamically adapts to visual contexts and generates multiple candidate
summaries conditioned on the input video. Extensive experiments demonstrate
that SummDiff not only achieves the state-of-the-art performance on various
benchmarks but also produces summaries that closely align with individual
annotator preferences. Moreover, we provide a deeper insight with novel metrics
from an analysis of the knapsack, which is an important last step of generating
summaries but has been overlooked in evaluation.

</details>


### [88] [In-Context Clustering with Large Language Models](https://arxiv.org/abs/2510.08466)
*Ying Wang,Mengye Ren,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 提出了In-Context Clustering (ICC)，一种基于LLM的灵活聚类方法，能够处理来自不同分布的数据。与传统聚类算法不同，ICC通过注意力机制灵活捕捉输入间的复杂关系，在文本编码的数值数据上展现出优秀的零样本聚类能力。


<details>
  <summary>Details</summary>
Motivation: 传统聚类算法受限于预定义的相似性度量，无法灵活捕捉复杂的数据关系。LLM的注意力机制和上下文学习能力为聚类任务提供了新的可能性。

Method: 使用预训练LLM的注意力矩阵进行谱聚类，并通过Next Token Prediction (NTP)损失对数值和图像数据进行微调，还实现了文本条件图像聚类。

Result: LLM在文本编码数值数据上展现出令人印象深刻的零样本聚类能力，注意力矩阵显示出明显的聚类模式。基于注意力矩阵的谱聚类性能具有竞争力。

Conclusion: 这项工作将上下文学习扩展到无监督设置，展示了LLM在聚类任务中的有效性和灵活性，为传统聚类方法提供了新的替代方案。

Abstract: We propose In-Context Clustering (ICC), a flexible LLM-based procedure for
clustering data from diverse distributions. Unlike traditional clustering
algorithms constrained by predefined similarity measures, ICC flexibly captures
complex relationships among inputs through an attention mechanism. We show that
pretrained LLMs exhibit impressive zero-shot clustering capabilities on
text-encoded numeric data, with attention matrices showing salient cluster
patterns. Spectral clustering using attention matrices offers surprisingly
competitive performance. We further enhance the clustering capabilities of LLMs
on numeric and image data through fine-tuning using the Next Token Prediction
(NTP) loss. Moreover, the flexibility of LLM prompting enables text-conditioned
image clustering, a capability that classical clustering methods lack. Our work
extends in-context learning to an unsupervised setting, showcasing the
effectiveness and flexibility of LLMs for clustering. Our code is available at
https://agenticlearning.ai/icc.

</details>


### [89] [Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models](https://arxiv.org/abs/2510.08492)
*Sharut Gupta,Shobhita Sundaram,Chenyu Wang,Stefanie Jegelka,Phillip Isola*

Main category: cs.LG

TL;DR: UML：一种模态无关的训练范式，通过交替处理不同模态的输入并共享参数，利用未配对的辅助多模态数据来增强目标模态的表征学习。


<details>
  <summary>Details</summary>
Motivation: 传统多模态学习依赖配对数据集，但忽略了利用未配对多模态数据直接增强目标模态表征学习的潜力。

Method: 提出UML训练范式，单一模型交替处理不同模态输入并共享参数，利用不同模态是共享底层现实投影的假设，无需显式配对即可受益于跨模态结构。

Result: 理论上在线性数据生成假设下，未配对辅助数据产生的表征比单模态训练更具信息性；实证表明使用文本、音频或图像等未配对辅助模态数据能持续提升图像和音频等单模态目标的下游性能。

Conclusion: UML证明了未配对多模态数据在增强单模态表征学习方面的有效性，为利用丰富但未配对的跨模态数据提供了新途径。

Abstract: Traditional multimodal learners find unified representations for tasks like
visual question answering, but rely heavily on paired datasets. However, an
overlooked yet potentially powerful question is: can one leverage auxiliary
unpaired multimodal data to directly enhance representation learning in a
target modality? We introduce UML: Unpaired Multimodal Learner, a
modality-agnostic training paradigm in which a single model alternately
processes inputs from different modalities while sharing parameters across
them. This design exploits the assumption that different modalities are
projections of a shared underlying reality, allowing the model to benefit from
cross-modal structure without requiring explicit pairs. Theoretically, under
linear data-generating assumptions, we show that unpaired auxiliary data can
yield representations strictly more informative about the data-generating
process than unimodal training. Empirically, we show that using unpaired data
from auxiliary modalities -- such as text, audio, or images -- consistently
improves downstream performance across diverse unimodal targets such as image
and audio. Our project page: https://unpaired-multimodal.github.io/

</details>


### [90] [Convergence Theorems for Entropy-Regularized and Distributional Reinforcement Learning](https://arxiv.org/abs/2510.08526)
*Yash Jhaveri,Harley Wiltzer,Patrick Shafto,Marc G. Bellemare,David Meger*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架，通过熵正则化和温度解耦策略，保证收敛到特定最优策略，实现可解释、保持多样性的最优策略。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在寻找最优策略时通常忽略学习策略的特性，难以表征将学习哪些策略及其行为。

Method: 使用熵正则化和温度解耦策略，在正则化温度趋近于零时实现可解释、保持多样性的最优策略。

Result: 该方法能保证收敛到特定最优策略，并确保策略衍生对象（价值函数和回报分布）的收敛性。

Conclusion: 提出的框架和算法能够估计与可解释、保持多样性的最优策略相关的回报分布，达到任意精度。

Abstract: In the pursuit of finding an optimal policy, reinforcement learning (RL)
methods generally ignore the properties of learned policies apart from their
expected return. Thus, even when successful, it is difficult to characterize
which policies will be learned and what they will do. In this work, we present
a theoretical framework for policy optimization that guarantees convergence to
a particular optimal policy, via vanishing entropy regularization and a
temperature decoupling gambit. Our approach realizes an interpretable,
diversity-preserving optimal policy as the regularization temperature vanishes
and ensures the convergence of policy derived objects--value functions and
return distributions. In a particular instance of our method, for example, the
realized policy samples all optimal actions uniformly. Leveraging our
temperature decoupling gambit, we present an algorithm that estimates, to
arbitrary accuracy, the return distribution associated to its interpretable,
diversity-preserving optimal policy.

</details>


### [91] [On the optimization dynamics of RLVR: Gradient gap and step size thresholds](https://arxiv.org/abs/2510.08539)
*Joe Suk,Yaqi Duan*

Main category: cs.LG

TL;DR: 本文为RLVR（带可验证奖励的强化学习）建立了理论框架，分析了其在完整响应和token级别的训练过程，提出了梯度间隙概念，证明了收敛性取决于更新方向与梯度间隙的对齐，并推导了基于梯度间隙大小的步长阈值。


<details>
  <summary>Details</summary>
Motivation: RLVR使用简单的二元反馈来后训练大语言模型，在实证中取得了显著成功，但缺乏对其工作原理的理论理解。

Method: 通过分析RLVR在完整响应（轨迹）和token级别的训练过程，引入梯度间隙概念来形式化从低奖励到高奖励区域的改进方向，并推导收敛条件和步长阈值。

Result: 证明了收敛性关键取决于更新方向与梯度间隙的对齐，推导了基于梯度间隙大小的步长阈值，并预测了临界步长如何随响应长度和成功率缩放。

Conclusion: 理论预测解释了为什么长度归一化等实用启发式方法能提高稳定性，并表明固定学习率下成功率可能停滞在100%以下。通过受控老虎机模拟和LLM实验验证了这些预测。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple
binary feedback to post-train large language models, has shown significant
empirical success. However, a principled understanding of why it works has been
lacking. This paper builds a theoretical foundation for RLVR by analyzing its
training process at both the full-response (trajectory) and token levels.
Central to our analysis is a quantity called the Gradient Gap, which formalizes
the direction of improvement from low-reward to high-reward regions of the
response space. We prove that convergence critically depends on aligning the
update direction with this Gradient Gap. Moreover, we derive a sharp step-size
threshold based on the magnitude of the Gradient Gap: below it, learning
converges, whereas above it, performance collapses. Our theory further predicts
how the critical step size must scale with response length and the success
rate, thereby explaining why practical heuristics such as length normalization
improve stability and showing that, with a fixed learning rate, the success
rate can stagnate strictly below $100\%$. We validate these predictions through
controlled bandit simulations and LLM experiments, including training
Qwen2.5-7B with GRPO.

</details>


### [92] [Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints](https://arxiv.org/abs/2510.08549)
*Zilin Kang,Chonghua Liao,Tingqiang Xu,Huazhe Xu*

Main category: cs.LG

TL;DR: ERA是一种通过设计特殊激活函数来约束模型输出采样熵的新范式，在多个领域都显著提升了性能，且计算开销小于7%。


<details>
  <summary>Details</summary>
Motivation: 为了解决模型输出熵控制的问题，提出通过设计特殊激活函数来约束采样熵，从而提升模型性能。

Method: 通过设计专门的激活函数应用于模型输出，约束采样熵不低于给定阈值。

Result: 在多个领域取得显著提升：LLM的AIME 2025分数提升37.4%，强化学习智能体性能提升30%以上，图像分类准确率提升0.69%。

Conclusion: 输出激活是熵控制的有效工具，为设计更简单和鲁棒的算法开辟了新方向。

Abstract: We propose ERA, a new paradigm that constrains the sampling entropy above
given thresholds by applying specially designed activations to the outputs of
models. Our approach demonstrates broad effectiveness across different domains:
1) for large language models(LLMs), boosting the AIME 2025 score for
Qwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning
agents, improving performance by more than 30% over strong baselines such as
SAC on the challenging HumanoidBench; 3) for image classification, enhancing
ImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a
computational overhead of less than 7%. Our work validates output activation as
a powerful tool for entropy control, opening a new direction for designing
simpler and more robust algorithms.

</details>


### [93] [Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization](https://arxiv.org/abs/2510.08554)
*Kevin Rojas,Jiahe Lin,Kashif Rasul,Anderson Schneider,Yuriy Nevmyvaka,Molei Tao,Wei Deng*

Main category: cs.LG

TL;DR: 本文提出了GDPO算法，一种针对扩散语言模型的强化学习优化方法，通过半确定性蒙特卡洛方案降低ELBO估计器的方差，在数学、推理和编程基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具有并行生成和迭代优化的优势，但现有的强化学习微调方法存在偏差大或计算成本高的问题，需要一种更有效的优化方法。

Method: 提出GDPO算法，通过分解ELBO估计的方差来源，使用半确定性蒙特卡洛方案减少方差，在有限计算预算下提供更低方差的估计器。

Result: GDPO在预训练检查点基础上取得一致提升，在大多数数学、推理和编程基准测试中优于当前最先进的diffu-GRPO方法。

Conclusion: GDPO为扩散语言模型的强化学习微调提供了有效的解决方案，通过降低ELBO估计方差实现了更好的性能表现。

Abstract: Diffusion language models (DLMs) enable parallel, order-agnostic generation
with iterative refinement, offering a flexible alternative to autoregressive
large language models (LLMs). However, adapting reinforcement learning (RL)
fine-tuning to DLMs remains an open challenge because of the intractable
likelihood. Pioneering work such as diffu-GRPO estimated token-level
likelihoods via one-step unmasking. While computationally efficient, this
approach is severely biased. A more principled foundation lies in
sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a
surrogate. Yet, despite this clean mathematical connection, ELBO-based methods
have seen limited adoption due to the prohibitive cost of likelihood
evaluation. In this work, we revisit ELBO estimation and disentangle its
sources of variance. This decomposition motivates reducing variance through
fast, deterministic integral approximations along a few pivotal dimensions.
Building on this insight, we introduce \textbf{Group Diffusion Policy
Optimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages
simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the
variance explosion of ELBO estimators under vanilla double Monte Carlo
sampling, yielding a provably lower-variance estimator under tight evaluation
budgets. Empirically, GDPO achieves consistent gains over pretrained
checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines,
on the majority of math, reasoning, and coding benchmarks.

</details>
